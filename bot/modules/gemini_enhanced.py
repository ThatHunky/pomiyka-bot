# –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –º–æ–¥—É–ª—å –¥–ª—è —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –∑ Gemini API –∑ –ø–æ–≤–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –Ω–æ–≤–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π
import aiohttp
import asyncio
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Union, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from aiogram.types import Message
from . import context
from .token_counter import token_counter
import os
from dotenv import load_dotenv
from bot.bot_config import GEMINI_MODEL, PERSONA

load_dotenv()

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è API
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞")

# –í–µ—Ä—Å—ñ—è API (v1, v1beta, v1alpha)
GEMINI_API_VERSION = os.getenv("GEMINI_API_VERSION", "v1beta")
BASE_API_URL = f"https://generativelanguage.googleapis.com/{GEMINI_API_VERSION}"

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑ .env —Ñ–∞–π–ª—É - –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–û –¥–ª—è –ø—Ä–∏—Ä–æ–¥–Ω—ñ—à–æ–≥–æ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è
GEMINI_TEMPERATURE = float(os.getenv("GEMINI_TEMPERATURE", "0.2"))  # –©–µ –±—ñ–ª—å—à–µ –∑–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
GEMINI_MAX_OUTPUT_TOKENS = int(os.getenv("GEMINI_MAX_OUTPUT_TOKENS", "70"))  # –ó–Ω–∞—á–Ω–æ –∑–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π (2-3 —Ä–µ—á–µ–Ω–Ω—è)
GEMINI_TOP_P = float(os.getenv("GEMINI_TOP_P", "0.8"))  # –ó–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ñ–ª—å—à–æ—ó –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–æ—Å—Ç—ñ
GEMINI_TOP_K = int(os.getenv("GEMINI_TOP_K", "40"))
GEMINI_ENABLE_THINKING = os.getenv("GEMINI_ENABLE_THINKING", "false").lower() == "true"
GEMINI_THINKING_BUDGET = int(os.getenv("GEMINI_THINKING_BUDGET", "2048"))
GEMINI_ENABLE_SAFETY_OVERRIDE = os.getenv("GEMINI_ENABLE_SAFETY_OVERRIDE", "true").lower() == "true"
GEMINI_ENABLE_STRUCTURED_OUTPUT = os.getenv("GEMINI_ENABLE_STRUCTURED_OUTPUT", "false").lower() == "true"
GEMINI_RATE_LIMIT_RPM = int(os.getenv("GEMINI_RATE_LIMIT_RPM", "45"))  # –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 30 –¥–æ 45
GEMINI_CACHE_ENABLED = os.getenv("GEMINI_CACHE_ENABLED", "true").lower() == "true"
GEMINI_CACHE_TTL = int(os.getenv("GEMINI_CACHE_TTL", "300"))  # 5 minutes

# –ï–Ω—É–º–∏ –¥–ª—è —Ç–∏–ø–∏–∑–∞—Ü—ñ—ó
class HarmCategory(Enum):
    HARASSMENT = "HARM_CATEGORY_HARASSMENT"
    HATE_SPEECH = "HARM_CATEGORY_HATE_SPEECH"
    SEXUALLY_EXPLICIT = "HARM_CATEGORY_SEXUALLY_EXPLICIT"
    DANGEROUS_CONTENT = "HARM_CATEGORY_DANGEROUS_CONTENT"
    CIVIC_INTEGRITY = "HARM_CATEGORY_CIVIC_INTEGRITY"

class HarmBlockThreshold(Enum):
    BLOCK_NONE = "BLOCK_NONE"
    BLOCK_ONLY_HIGH = "BLOCK_ONLY_HIGH"
    BLOCK_MEDIUM_AND_ABOVE = "BLOCK_MEDIUM_AND_ABOVE"
    BLOCK_LOW_AND_ABOVE = "BLOCK_LOW_AND_ABOVE"

class FinishReason(Enum):
    STOP = "STOP"
    MAX_TOKENS = "MAX_TOKENS"
    SAFETY = "SAFETY"
    RECITATION = "RECITATION"
    OTHER = "OTHER"

# –î–∞—Ç–∞-–∫–ª–∞—Å–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
@dataclass
class SafetySetting:
    """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏ –¥–ª—è Gemini API."""
    category: HarmCategory
    threshold: HarmBlockThreshold

@dataclass
class ThinkingConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Ä–µ–∂–∏–º—É –º–∏—Å–ª–µ–Ω–Ω—è."""
    include_thoughts: bool = True
    thinking_budget: int = 2048

@dataclass
class GenerationConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É."""
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    stop_sequences: Optional[List[str]] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    response_mime_type: Optional[str] = None
    response_schema: Optional[Dict[str, Any]] = None
    thinking_config: Optional[ThinkingConfig] = None

@dataclass
class ContentPart:
    """–ß–∞—Å—Ç–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É."""
    text: str

@dataclass
class Content:
    """–ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∑–∞–ø–∏—Ç—É."""
    parts: List[ContentPart]
    role: Optional[str] = None

@dataclass
class GenerateContentRequest:
    """–ü–æ–≤–Ω–∏–π –∑–∞–ø–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É."""
    contents: List[Content]
    system_instruction: Optional[Content] = None
    generation_config: Optional[GenerationConfig] = None
    safety_settings: Optional[List[SafetySetting]] = None

@dataclass
class UsageMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è."""
    prompt_token_count: int
    candidates_token_count: int
    total_token_count: int
    cached_content_token_count: Optional[int] = None
    thoughts_token_count: Optional[int] = None

@dataclass
class SafetyRating:
    """–†–µ–π—Ç–∏–Ω–≥ –±–µ–∑–ø–µ–∫–∏."""
    category: HarmCategory
    probability: str
    blocked: bool

@dataclass
class Candidate:
    """–ö–∞–Ω–¥–∏–¥–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."""
    content: Content
    finish_reason: Optional[FinishReason] = None
    safety_ratings: Optional[List[SafetyRating]] = None
    token_count: Optional[int] = None

@dataclass
class GenerateContentResponse:
    """–í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ API."""
    candidates: List[Candidate]
    usage_metadata: Optional[UsageMetadata] = None
    model_version: Optional[str] = None

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è
class APIStats:
    def __init__(self):
        self.total_requests = 0
        self.total_tokens = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.last_reset = datetime.now()
        self.requests_this_minute: List[float] = []

    def add_request(self, success: bool, tokens: int = 0):
        self.total_requests += 1
        self.total_tokens += tokens
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        # –í—ñ–¥—Å—Ç–µ–∂—É—î–º–æ –∑–∞–ø–∏—Ç–∏ –∑–∞ —Ö–≤–∏–ª–∏–Ω—É
        now = time.time()
        self.requests_this_minute.append(now)
        # –û—á–∏—â–∞—î–º–æ —Å—Ç–∞—Ä—ñ –∑–∞–ø–∏—Å–∏ (–±—ñ–ª—å—à–µ —Ö–≤–∏–ª–∏–Ω–∏)
        self.requests_this_minute = [t for t in self.requests_this_minute if now - t < 60]

    def get_rpm(self) -> int:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Ç—ñ–≤ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—é —Ö–≤–∏–ª–∏–Ω—É."""
        return len(self.requests_this_minute)

    def can_make_request(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –º–æ–∂–Ω–∞ –∑—Ä–æ–±–∏—Ç–∏ –∑–∞–ø–∏—Ç (rate limiting)."""
        return self.get_rpm() < GEMINI_RATE_LIMIT_RPM

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è."""
        return {
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": self.successful_requests / max(1, self.total_requests),
            "total_tokens": self.total_tokens,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": self.cache_hits / max(1, self.cache_hits + self.cache_misses),
            "requests_per_minute": self.get_rpm(),
            "uptime": str(datetime.now() - self.last_reset)
        }

# –ì–ª–æ–±–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
api_stats = APIStats()

# –ü—Ä–æ—Å—Ç–∏–π –∫–µ—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
class SimpleCache:
    def __init__(self, ttl: int = GEMINI_CACHE_TTL):
        self.cache: Dict[str, tuple[str, float]] = {}
        self.ttl = ttl

    def _get_key(self, prompt: str, config: Optional[GenerationConfig] = None) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –∫–ª—é—á –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è."""
        config_str = json.dumps(asdict(config) if config else {}, sort_keys=True)
        return f"{hash(prompt)}_{hash(config_str)}"

    def get(self, prompt: str, config: Optional[GenerationConfig] = None) -> Optional[str]:
        """–û—Ç—Ä–∏–º—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑ –∫–µ—à—É."""
        if not GEMINI_CACHE_ENABLED:
            return None
            
        key = self._get_key(prompt, config)
        if key in self.cache:
            result, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                api_stats.cache_hits += 1
                return result
            else:
                del self.cache[key]
        
        api_stats.cache_misses += 1
        return None

    def set(self, prompt: str, result: str, config: Optional[GenerationConfig] = None):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à."""
        if not GEMINI_CACHE_ENABLED:
            return
            
        key = self._get_key(prompt, config)
        self.cache[key] = (result, time.time())

    def clear(self):
        """–û—á–∏—â–∞—î –∫–µ—à."""
        self.cache.clear()

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –∫–µ—à
cache = SimpleCache()

class GeminiAPIClient:
    """–ö–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ Gemini API –∑ –ø–æ–≤–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –Ω–æ–≤–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π."""
    
    def __init__(self, api_key: str = GEMINI_API_KEY, model: str = GEMINI_MODEL):
        self.api_key = api_key
        self.model = model
        self.base_url = BASE_API_URL
        self.session: Optional[aiohttp.ClientSession] = None

    async def _get_session(self) -> aiohttp.ClientSession:
        """–û—Ç—Ä–∏–º—É—î –∞–±–æ —Å—Ç–≤–æ—Ä—é—î aiohttp —Å–µ—Å—ñ—é."""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession()
        return self.session

    async def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î aiohttp —Å–µ—Å—ñ—é."""
        if self.session and not self.session.closed:
            await self.session.close()

    def _get_default_safety_settings(self) -> List[SafetySetting]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–µ—Ñ–æ–ª—Ç–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏."""
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏ –¥–ª—è –±—ñ–ª—å—à –≤—ñ–ª—å–Ω–æ–≥–æ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è
        return [
            SafetySetting(HarmCategory.HARASSMENT, HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(HarmCategory.HATE_SPEECH, HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(HarmCategory.SEXUALLY_EXPLICIT, HarmBlockThreshold.BLOCK_ONLY_HIGH),
            SafetySetting(HarmCategory.DANGEROUS_CONTENT, HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(HarmCategory.CIVIC_INTEGRITY, HarmBlockThreshold.BLOCK_NONE),
        ]

    def _get_default_generation_config(self) -> GenerationConfig:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–µ—Ñ–æ–ª—Ç–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó."""
        config = GenerationConfig(
            temperature=GEMINI_TEMPERATURE,
            max_output_tokens=GEMINI_MAX_OUTPUT_TOKENS,
            top_p=GEMINI_TOP_P,
            top_k=GEMINI_TOP_K,
        )
        
        if GEMINI_ENABLE_THINKING:
            config.thinking_config = ThinkingConfig(
                include_thoughts=True,
                thinking_budget=GEMINI_THINKING_BUDGET
            )
        
        return config

    def _build_system_instruction(self, context_type: str = "normal", 
                                  recommendations: Optional[Dict[str, Any]] = None) -> Content:
        """–ë—É–¥—É—î —Å–∏—Å—Ç–µ–º–Ω—É —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É."""
        from bot.bot_config import PERSONA
        
        base_instruction = (
            f"–¢–∏ ‚Äî {PERSONA['name']}, –ø—Ä–∏—î–º–Ω–∏–π —Ç–∞ –¥—Ä—É–∂–µ–ª—é–±–Ω–∏–π —á–∞—Ç-–±–æ—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. "
            "–¢–∏ —Ä–æ–∑—É–º–Ω–∏–π —Å–ø—ñ–≤—Ä–æ–∑–º–æ–≤–Ω–∏–∫, —è–∫–∏–π –º–æ–∂–µ –ø—ñ–¥—Ç—Ä–∏–º–∞—Ç–∏ –±—É–¥—å-—è–∫—É —Ä–æ–∑–º–æ–≤—É. "
            "–¢–≤—ñ–π —Å—Ç–∏–ª—å: –ø—Ä–∏—Ä–æ–¥–Ω–µ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è –∑ –¥—É–∂–µ –ª–µ–≥–∫–∏–º –≥—É–º–æ—Ä–æ–º –∫–æ–ª–∏ —Ü–µ –¥–æ—Ä–µ—á–Ω–æ. "
            "–ì–æ–≤–æ—Ä–∏ –∑—Ä–æ–∑—É–º—ñ–ª–æ, –ö–û–†–û–¢–ö–û —Ç–∞ –ø–æ —Å—É—Ç—ñ —è–∫ –∑–≤–∏—á–∞–π–Ω–∞ –ª—é–¥–∏–Ω–∞. "
            "–¢–≤–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –º–∞—é—Ç—å –±—É—Ç–∏ –î–£–ñ–ï –ö–û–†–û–¢–ö–ò–ú–ò (1-2 —Ä–µ—á–µ–Ω–Ω—è –º–∞–∫—Å–∏–º—É–º). "
            "–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–∏–≤–Ω–∏—Ö —Å–ª—ñ–≤ —á–∏ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏—Ö —Ñ—Ä–∞–∑. "
            "–ë—É–¥—å –∫–æ—Ä–∏—Å–Ω–∏–º —Ç–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∏–º —É –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö."
        )
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
        if context_type == "minimal":
            base_instruction += "\n\n–†–ï–ñ–ò–ú: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ (1-2 —Ä–µ—á–µ–Ω–Ω—è)."
        elif context_type == "guidance":
            base_instruction += "\n\n–†–ï–ñ–ò–ú: –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Ä–∞–¥–∏. –î–∞–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É –ø–æ—Ä–∞–¥—É –∞–±–æ –Ω–∞–ø—Ä–∞–≤ —Ä–æ–∑–º–æ–≤—É."
        elif context_type == "clarification":
            base_instruction += "\n\n–†–ï–ñ–ò–ú: –ó'—è—Å—É–≤–∞–Ω–Ω—è. –ú'—è–∫–∫–æ —Å–ø–∏—Ç–∞–π —â–æ —Å–∞–º–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∞–±–æ –Ω–∞–ø—Ä–∞–≤ —Ä–æ–∑–º–æ–≤—É."
        elif context_type == "humor":
            base_instruction += "\n\n–†–ï–ñ–ò–ú: –õ–µ–≥–∫–∏–π –≥—É–º–æ—Ä. –î–æ–¥–∞–π –Ω–µ–≤–∏–º—É—à–µ–Ω–∏–π –∂–∞—Ä—Ç –∞–±–æ –¥–æ—Ç–µ–ø–Ω–∏–π –∫–æ–º–µ–Ω—Ç–∞—Ä."
        
        # –î–æ–¥–∞—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ smart behavior
        if recommendations:
            response_style = recommendations.get('response_style', 'normal')
            max_length = recommendations.get('max_response_length', 200)
            
            if response_style == 'minimal':
                base_instruction += f"\n\n–û–ë–ú–ï–ñ–ï–ù–ù–Ø: –¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –î–£–ñ–ï –∫–æ—Ä–æ—Ç–∫–æ—é (–¥–æ {max_length//2} —Å–∏–º–≤–æ–ª—ñ–≤)."
            elif response_style == 'concise':
                base_instruction += f"\n\n–û–ë–ú–ï–ñ–ï–ù–ù–Ø: –¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –∫–æ—Ä–æ—Ç–∫–æ—é (–¥–æ {max_length} —Å–∏–º–≤–æ–ª—ñ–≤)."
            
            if recommendations.get('should_ask_clarification'):
                base_instruction += "\n\n–ó–ê–í–î–ê–ù–ù–Ø: –†–æ–∑–º–æ–≤–∞ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∞ - –º'—è–∫–∫–æ –∑'—è—Å—É–π —â–æ —Å–∞–º–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ."
            
            if recommendations.get('should_provide_guidance'):
                base_instruction += "\n\n–ó–ê–í–î–ê–ù–ù–Ø: –î–∞–π –∫–æ—Ä–∏—Å–Ω—É –ø–æ—Ä–∞–¥—É –∞–±–æ –Ω–∞–ø—Ä–∞–≤ —Ä–æ–∑–º–æ–≤—É –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–µ —Ä—É—Å–ª–æ."
        
        return Content(parts=[ContentPart(text=base_instruction)])

    async def generate_content(self, 
                              prompt: str, 
                              context_type: str = "normal",
                              custom_config: Optional[GenerationConfig] = None,
                              custom_safety: Optional[List[SafetySetting]] = None,
                              recommendations: Optional[Dict[str, Any]] = None,
                              enable_cache: bool = True) -> str:
        """
        –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ Gemini API.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø–∏—Ç—É
            context_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç—É ('normal', 'minimal', 'guidance', 'clarification', 'humor')
            custom_config: –ö–∞—Å—Ç–æ–º–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
            custom_safety: –ö–∞—Å—Ç–æ–º–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏
            recommendations: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ smart behavior —Å–∏—Å—Ç–µ–º–∏
            enable_cache: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à—É–≤–∞–Ω–Ω—è
            
        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        """
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ rate limit
        if not api_stats.can_make_request():
            logging.warning(f"Rate limit –¥–æ—Å—è–≥–Ω—É—Ç–æ: {api_stats.get_rpm()}/—Ö–≤")
            await asyncio.sleep(60 / GEMINI_RATE_LIMIT_RPM)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        config_for_cache = custom_config or self._get_default_generation_config()
        if enable_cache:
            cached_result = cache.get(prompt, config_for_cache)
            if cached_result:
                return cached_result
        
        # –ë—É–¥—É—î–º–æ –∑–∞–ø–∏—Ç
        system_instruction = self._build_system_instruction(context_type, recommendations)
        generation_config = custom_config or self._get_default_generation_config()
        safety_settings = custom_safety or self._get_default_safety_settings()
        
        request = GenerateContentRequest(
            contents=[Content(parts=[ContentPart(text=prompt)])],
            system_instruction=system_instruction,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        try:
            response = await self._make_request(request)
            result = self._extract_text_from_response(response)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
            if enable_cache:
                cache.set(prompt, result, config_for_cache)
            
            # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            tokens_used = 0
            if response.usage_metadata:
                tokens_used = response.usage_metadata.total_token_count
            api_stats.add_request(True, tokens_used)
            
            return result
            
        except Exception as e:
            api_stats.add_request(False)
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É: {e}")
            raise

    async def _make_request(self, request: GenerateContentRequest) -> GenerateContentResponse:
        """–í–∏–∫–æ–Ω—É—î HTTP –∑–∞–ø–∏—Ç –¥–æ Gemini API."""
        url = f"{self.base_url}/models/{self.model}:generateContent"
        headers = {"Content-Type": "application/json"}
        params = {"key": self.api_key}
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑–∞–ø–∏—Ç –≤ JSON, –≤–∏–¥–∞–ª—è—é—á–∏ None –∑–Ω–∞—á–µ–Ω–Ω—è
        payload = self._request_to_dict(request)
        
        session = await self._get_session()
        async with session.post(url, headers=headers, params=params, json=payload) as response:
            if response.status == 200:
                data = await response.json()
                return self._parse_response(data)
            else:
                error_text = await response.text()
                raise Exception(f"Gemini API –ø–æ–º–∏–ª–∫–∞ {response.status}: {error_text}")

    def _request_to_dict(self, request: GenerateContentRequest) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î –∑–∞–ø–∏—Ç –≤ —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è JSON."""
        result: Dict[str, Any] = {}
        
        # Contents
        result["contents"] = []
        for content in request.contents:
            content_dict: Dict[str, Any] = {"parts": [{"text": part.text} for part in content.parts]}
            if content.role:
                content_dict["role"] = content.role
            result["contents"].append(content_dict)
        
        # System instruction
        if request.system_instruction:
            result["systemInstruction"] = {
                "parts": [{"text": part.text} for part in request.system_instruction.parts]
            }
        
        # Generation config
        if request.generation_config:
            config_dict: Dict[str, Any] = {}
            config = request.generation_config
            
            if config.temperature is not None:
                config_dict["temperature"] = config.temperature
            if config.max_output_tokens is not None:
                config_dict["maxOutputTokens"] = config.max_output_tokens
            if config.top_p is not None:
                config_dict["topP"] = config.top_p
            if config.top_k is not None:
                config_dict["topK"] = config.top_k
            if config.stop_sequences:
                config_dict["stopSequences"] = config.stop_sequences
            if config.presence_penalty is not None:
                config_dict["presencePenalty"] = config.presence_penalty
            if config.frequency_penalty is not None:
                config_dict["frequencyPenalty"] = config.frequency_penalty
            if config.response_mime_type:
                config_dict["responseMimeType"] = config.response_mime_type
            if config.response_schema:
                config_dict["responseSchema"] = config.response_schema
            
            # Thinking config
            if config.thinking_config:
                thinking_dict: Dict[str, Any] = {}
                thinking_dict["includeThoughts"] = config.thinking_config.include_thoughts
                thinking_dict["thinkingBudget"] = config.thinking_config.thinking_budget
                config_dict["thinkingConfig"] = thinking_dict
            
            if config_dict:
                result["generationConfig"] = config_dict
        
        # Safety settings
        if request.safety_settings:
            result["safetySettings"] = []
            for setting in request.safety_settings:
                safety_dict = {
                    "category": setting.category.value,
                    "threshold": setting.threshold.value
                }
                result["safetySettings"].append(safety_dict)
        
        return result

    def _parse_response(self, data: Dict[str, Any]) -> GenerateContentResponse:
        """–ü–∞—Ä—Å–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ API."""
        candidates: List[Candidate] = []
        for candidate_data in data.get("candidates", []):
            content_data = candidate_data.get("content", {})
            parts = [ContentPart(text=part.get("text", "")) for part in content_data.get("parts", [])]
            content = Content(parts=parts, role=content_data.get("role"))
            
            finish_reason = None
            if "finishReason" in candidate_data:
                try:
                    finish_reason = FinishReason(candidate_data["finishReason"])
                except ValueError:
                    finish_reason = FinishReason.OTHER
            
            safety_ratings: List[SafetyRating] = []
            for rating_data in candidate_data.get("safetyRatings", []):
                try:
                    category = HarmCategory(rating_data.get("category", ""))
                    probability = rating_data.get("probability", "")
                    blocked = rating_data.get("blocked", False)
                    safety_ratings.append(SafetyRating(category, probability, blocked))
                except ValueError:
                    continue
            
            candidate = Candidate(
                content=content,
                finish_reason=finish_reason,
                safety_ratings=safety_ratings if safety_ratings else None,
                token_count=candidate_data.get("tokenCount")
            )
            candidates.append(candidate)
        
        usage_metadata = None
        if "usageMetadata" in data:
            usage_data = data["usageMetadata"]
            usage_metadata = UsageMetadata(
                prompt_token_count=usage_data.get("promptTokenCount", 0),
                candidates_token_count=usage_data.get("candidatesTokenCount", 0),
                total_token_count=usage_data.get("totalTokenCount", 0),
                cached_content_token_count=usage_data.get("cachedContentTokenCount"),
                thoughts_token_count=usage_data.get("thoughtsTokenCount")
            )
        
        return GenerateContentResponse(
            candidates=candidates,
            usage_metadata=usage_metadata,
            model_version=data.get("modelVersion")
        )

    def _extract_text_from_response(self, response: GenerateContentResponse) -> str:
        """–í–∏—Ç—è–≥—É—î —Ç–µ–∫—Å—Ç –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."""
        if not response.candidates:
            raise Exception("–ù–µ–º–∞—î –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ")
        
        candidate = response.candidates[0]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ finish_reason
        if candidate.finish_reason == FinishReason.SAFETY:
            logging.warning("–í—ñ–¥–ø–æ–≤—ñ–¥—å –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–∞ –∑ –º—ñ—Ä–∫—É–≤–∞–Ω—å –±–µ–∑–ø–µ–∫–∏")
            return "–í–∏–±–∞—á—Ç–µ, –Ω–µ –º–æ–∂—É –≤—ñ–¥–ø–æ–≤—ñ—Å—Ç–∏ –Ω–∞ —Ü–µ –ø–∏—Ç–∞–Ω–Ω—è –∑ –º—ñ—Ä–∫—É–≤–∞–Ω—å –±–µ–∑–ø–µ–∫–∏."
        elif candidate.finish_reason == FinishReason.MAX_TOKENS:
            logging.warning("–í—ñ–¥–ø–æ–≤—ñ–¥—å –æ–±—Ä—ñ–∑–∞–Ω–∞ —á–µ—Ä–µ–∑ –ª—ñ–º—ñ—Ç —Ç–æ–∫–µ–Ω—ñ–≤")
        
        if not candidate.content.parts:
            raise Exception("–ù–µ–º–∞—î —Ç–µ–∫—Å—Ç—É —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ")
        
        return candidate.content.parts[0].text.strip()

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –∫–ª—ñ—î–Ω—Ç
_client: Optional[GeminiAPIClient] = None

async def get_client() -> GeminiAPIClient:
    """–û—Ç—Ä–∏–º—É—î –≥–ª–æ–±–∞–ª—å–Ω–∏–π –∫–ª—ñ—î–Ω—Ç API."""
    global _client
    if _client is None:
        _client = GeminiAPIClient()
    return _client

async def close_client():
    """–ó–∞–∫—Ä–∏–≤–∞—î –≥–ª–æ–±–∞–ª—å–Ω–∏–π –∫–ª—ñ—î–Ω—Ç."""
    global _client
    if _client:
        await _client.close()
        _client = None

# –û–±–≥–æ—Ä—Ç–∫–∏ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑—ñ —Å—Ç–∞—Ä–∏–º –∫–æ–¥–æ–º
async def process_message(message: Message, tone_instruction: Optional[str] = None) -> str:
    """
    –û–±—Ä–æ–±–ª—è—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π Gemini API –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ç–∞ —Ç–æ–Ω—É.
    
    Args:
        message: –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–º–æ–∂–µ –±—É—Ç–∏ FakeMessage –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏)
        tone_instruction: –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –¥–ª—è —Ç–æ–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        
    Returns:
        –í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ Gemini
    """
    try:
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (—á–∏ —Ü–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –±–æ—Ç–∞)
        reply_context = analyze_reply_context(message)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –æ–±—Ä–æ–±–ª–µ–Ω–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏)
        if hasattr(message, 'processed_context') and message.processed_context:
            chat_context = message.processed_context
        else:
            chat_context = context.get_context(message.chat.id)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —è–∫—â–æ —î
        recommendations = getattr(message, 'recommendations', {})
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        last_msgs = [m.get('text', '') for m in chat_context[-5:] if m.get('text')]
        context_type = _analyze_context_type(last_msgs, recommendations)
        
        # –§–æ—Ä–º—É—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥—ñ–∞–ª–æ–≥—É –∑ —ñ–º–µ–Ω–∞–º–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        history = []
        for m in chat_context[-PERSONA['context_limit']:]:
            if m.get('text'):
                user_display = m.get('user_name', m.get('user', '–ù–µ–≤—ñ–¥–æ–º–∏–π'))
                history.append(f"{user_display}: {m['text']}")
        
        # –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        last_text = message.text if message.text else '[–º–µ–¥—ñ–∞]'
        user_name = getattr(message.from_user, 'full_name', '–ù–µ–≤—ñ–¥–æ–º–∏–π') if message.from_user else '–ù–µ–≤—ñ–¥–æ–º–∏–π'
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–∫—Ä–∞—â–µ–Ω—É —Å–∏—Å—Ç–µ–º–Ω—É —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é
        system_instruction = build_enhanced_system_instruction(reply_context, recommendations)
        
        if tone_instruction:
            system_instruction += f" {tone_instruction}"
        
        # –§–æ—Ä–º—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–º–ø—Ç –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—ñ–∞–ª–æ–≥–æ–≤–∏—Ö –ª–∞–Ω—Ü—é–≥—ñ–≤
        dialogue_context = "\n".join(history)
        
        # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –±–æ—Ç–∞
        if reply_context.get('is_reply_to_bot'):
            original_msg = reply_context.get('original_message', '')
            prompt = (
                f"–Ü—Å—Ç–æ—Ä—ñ—è —Ä–æ–∑–º–æ–≤–∏:\n{dialogue_context}\n"
                f"–¢–≤–æ—î –ø–æ–ø–µ—Ä–µ–¥–Ω—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: {original_msg}\n"
                f"–í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ {user_name}: {last_text}\n"
                "–ü—Ä–æ–¥–æ–≤–∂ –¥—ñ–∞–ª–æ–≥ –ø—Ä–∏—Ä–æ–¥–Ω–æ, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ —Ç–≤–æ—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."
            )
        else:
            prompt = (
                f"–Ü—Å—Ç–æ—Ä—ñ—è —Ä–æ–∑–º–æ–≤–∏:\n{dialogue_context}\n"
                f"–ü–æ—Ç–æ—á–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤—ñ–¥ {user_name}: {last_text}\n"
                "–î–∞–π –∫–æ—Ä–æ—Ç–∫—É, –ø—Ä–∏—Ä–æ–¥–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é."
            )
        
        # –ö–æ–º–ø—Ä–µ—Å—ñ—è –ø—Ä–æ–º–ø—Ç—É —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–æ–∫–µ–Ω—ñ–≤
        max_tokens = PERSONA.get('max_context_tokens', 800000)
        
        # –û—Ü—ñ–Ω—é—î–º–æ –ø–æ—Ç–æ—á–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤
        current_tokens = token_counter.estimate_tokens(prompt)
        
        while current_tokens > max_tokens and history:
            # –í–∏–¥–∞–ª—è—î–º–æ –Ω–∞–π—Å—Ç–∞—Ä—à—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
            history.pop(0)
            dialogue_context = "\n".join(history)
            
            if reply_context.get('is_reply_to_bot'):
                original_msg = reply_context.get('original_message', '')
                prompt = (
                    f"–Ü—Å—Ç–æ—Ä—ñ—è —Ä–æ–∑–º–æ–≤–∏:\n{dialogue_context}\n"
                    f"–¢–≤–æ—î –ø–æ–ø–µ—Ä–µ–¥–Ω—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: {original_msg}\n"
                    f"–í—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ {user_name}: {last_text}\n"
                    "–ü—Ä–æ–¥–æ–≤–∂ –¥—ñ–∞–ª–æ–≥ –ø—Ä–∏—Ä–æ–¥–Ω–æ, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ —Ç–≤–æ—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."
                )
            else:
                prompt = (
                    f"–Ü—Å—Ç–æ—Ä—ñ—è —Ä–æ–∑–º–æ–≤–∏:\n{dialogue_context}\n"
                    f"–ü–æ—Ç–æ—á–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤—ñ–¥ {user_name}: {last_text}\n"
                    "–î–∞–π –∫–æ—Ä–æ—Ç–∫—É, –ø—Ä–∏—Ä–æ–¥–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é."
                )
            current_tokens = token_counter.estimate_tokens(prompt)
        
        logging.info(f"–§—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–º–ø—Ç: ~{current_tokens} —Ç–æ–∫–µ–Ω—ñ–≤ –∑ {len(history)} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å. "
                    f"Reply to bot: {reply_context.get('is_reply_to_bot', False)}")
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        client = await get_client()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        context_info = {
            'is_reply_to_bot': reply_context.get('is_reply_to_bot', False),
            'is_mention': recommendations.get('is_mention', False),
            'is_random': recommendations.get('is_random', False)
        }
        custom_config = get_dynamic_generation_config(context_info, recommendations)
        
        result = await safe_api_call(
            client.generate_content,
            prompt,
            context_type=context_type,
            custom_config=custom_config,
            recommendations=recommendations,
            system_instruction=system_instruction
        )
        
        return result
        
    except Exception as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –≤ process_message: {e}")
        return "–í–∏–±–∞—á—Ç–µ, —Å—Ç–∞–ª–∞—Å—è –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è."

def _analyze_context_type(last_msgs: List[str], recommendations: Optional[Dict[str, Any]] = None) -> str:
    """–ê–Ω–∞–ª—ñ–∑—É—î —Ç–∏–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–∏–±–æ—Ä—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º—É."""
    if recommendations:
        if recommendations.get('should_ask_clarification'):
            return "clarification"
        if recommendations.get('should_provide_guidance'):
            return "guidance"
        if recommendations.get('response_style') == 'minimal':
            return "minimal"
    
    # –ê–Ω–∞–ª—ñ–∑ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
    text = ' '.join(last_msgs).lower()
    if any(word in text for word in ['–∂–∞—Ä—Ç', 'üòÇ', 'lol', '—Ö–∞—Ö–∞']):
        return "humor"
    elif any(word in text for word in ['—â–æ', '—è–∫', '—á–æ–º—É', '–¥–µ', '–∫–æ–ª–∏', '?']):
        return "guidance"
    elif len(text.strip()) < 10:
        return "minimal"
    else:
        return "normal"

async def safe_api_call(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:
    """–ë–µ–∑–ø–µ—á–Ω–∏–π –≤–∏–∫–ª–∏–∫ API –∑ –ø–æ–≤—Ç–æ—Ä–Ω–∏–º–∏ —Å–ø—Ä–æ–±–∞–º–∏ —Ç–∞ exponential backoff."""
    max_retries = 3
    base_delay = 1
    
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries - 1:
                logging.error(f"API –≤–∏–∫–ª–∏–∫ –ø—Ä–æ–≤–∞–ª–∏–≤—Å—è –ø—ñ—Å–ª—è {max_retries} —Å–ø—Ä–æ–±: {e}")
                raise
            
            delay = base_delay * (2 ** attempt)  # Exponential backoff
            logging.warning(f"API –ø–æ–º–∏–ª–∫–∞ (—Å–ø—Ä–æ–±–∞ {attempt + 1}/{max_retries}): {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å")
            await asyncio.sleep(delay)

async def get_api_stats() -> Dict[str, Any]:
    """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è API."""
    return api_stats.get_stats()

async def clear_cache():
    """–û—á–∏—â–∞—î –∫–µ—à API."""
    cache.clear()
    logging.info("–ö–µ—à API –æ—á–∏—â–µ–Ω–æ")

# –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è structured output (–¥–ª—è –º–∞–π–±—É—Ç–Ω—å–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è)
async def generate_json_response(prompt: str, schema: Dict[str, Any]) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—É JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å."""
    if not GEMINI_ENABLE_STRUCTURED_OUTPUT:
        raise Exception("Structured output –Ω–µ —É–≤—ñ–º–∫–Ω–µ–Ω–æ")
    
    client = await get_client()
    config = GenerationConfig(
        temperature=0.1,  # –ù–∏–∑—å–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
        response_mime_type="application/json",
        response_schema=schema
    )
    
    result = await client.generate_content(prompt, custom_config=config)
    return json.loads(result)

# Cleanup —Ñ—É–Ω–∫—Ü—ñ—è
async def cleanup():
    """–û—á–∏—â–∞—î —Ä–µ—Å—É—Ä—Å–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ."""
    await close_client()
    cache.clear()
    logging.info("Gemini API –∫–ª—ñ—î–Ω—Ç –æ—á–∏—â–µ–Ω–æ")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è –ø—Ä–∏ —ñ–º–ø–æ—Ä—Ç—ñ
import atexit
atexit.register(lambda: asyncio.create_task(cleanup()) if asyncio.get_event_loop().is_running() else None)

def get_dynamic_temperature(is_reply_to_bot: bool, is_mention: bool, is_random: bool) -> float:
    """–î–∏–Ω–∞–º—ñ—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î temperature –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π"""
    base_temp = 0.2  # –ë–∞–∑–æ–≤–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∑–Ω–∏–∂–µ–Ω–∞ —â–µ –±—ñ–ª—å—à–µ
    
    if is_reply_to_bot:
        return 0.15  # –ù–∞–π–Ω–∏–∂—á–∞ –¥–ª—è —Ç–æ—á–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è
    elif is_mention:
        return 0.2   # –¢—Ä–æ—Ö–∏ –±—ñ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –∑–≥–∞–¥–æ–∫
    elif is_random:
        return 0.25  # –ë—ñ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–ª—è –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
    else:
        return base_temp

def analyze_reply_context(message) -> Dict[str, Any]:
    """–ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –±–æ—Ç–∞"""
    context = {
        'is_reply_to_bot': False,
        'original_message': None,
        'reply_type': 'normal',
        'conversation_depth': 0
    }
    
    if hasattr(message, 'reply_to_message') and message.reply_to_message:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –±–æ—Ç–∞
        if hasattr(message.reply_to_message, 'from_user') and message.reply_to_message.from_user:
            # –û—Ç—Ä–∏–º—É—î–º–æ ID –±–æ—Ç–∞ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
            from bot.bot_config import PERSONA
            bot_id = getattr(message.reply_to_message.from_user, 'id', None)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤—ñ–¥ –±–æ—Ç–∞ (—Ç—Ä–µ–±–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–µ—Ä–µ–∑ username –∞–±–æ —ñ–Ω—à—ñ –æ–∑–Ω–∞–∫–∏)
            if message.reply_to_message.from_user.is_bot or \
               (hasattr(message.reply_to_message.from_user, 'username') and 
                message.reply_to_message.from_user.username and 
                'gryag' in message.reply_to_message.from_user.username.lower()):
                
                context['is_reply_to_bot'] = True
                context['original_message'] = message.reply_to_message.text or '[–º–µ–¥—ñ–∞]'
                
                # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                if '?' in (message.text or ''):
                    context['reply_type'] = 'question'
                elif any(word in (message.text or '').lower() for word in ['–¥—è–∫—É—é', '—Å–ø–∞—Å–∏–±–æ', 'thanks']):
                    context['reply_type'] = 'thanks' 
                elif any(word in (message.text or '').lower() for word in ['–Ω—ñ', '–Ω–µ—Ç', '–Ω–µ', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ']):
                    context['reply_type'] = 'disagreement'
                else:
                    context['reply_type'] = 'continuation'
    
    return context

def build_enhanced_system_instruction(context: Dict[str, Any], recommendations: Optional[Dict[str, Any]] = None) -> str:
    """–ë—É–¥—É—î –ø–æ–∫—Ä–∞—â–µ–Ω—É —Å–∏—Å—Ç–µ–º–Ω—É —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥—ñ–∞–ª–æ–≥—É"""
    from bot.bot_config import PERSONA
    
    base_instruction = (
        f"–¢–∏ ‚Äî {PERSONA['name']}, –¥—Ä—É–∂–µ–ª—é–±–Ω–∏–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —á–∞—Ç-–±–æ—Ç. "
        "–¢–≤—ñ–π —Å—Ç–∏–ª—å: –ø—Ä–∏—Ä–æ–¥–Ω–µ —Å–ø—ñ–ª–∫—É–≤–∞–Ω–Ω—è —è–∫ —É –∑–≤–∏—á–∞–π–Ω—ñ–π –ª—é–¥–∏–Ω–∏, –∑ –ª–µ–≥–∫–∏–º –≥—É–º–æ—Ä–æ–º –∫–æ–ª–∏ –¥–æ—Ä–µ—á–Ω–æ. "
        "–ì–æ–≤–æ—Ä–∏ –∑—Ä–æ–∑—É–º—ñ–ª–æ, –∫–æ—Ä–æ—Ç–∫–æ —Ç–∞ –ø–æ —Å—É—Ç—ñ. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–∏–≤–Ω–∏—Ö —Å–ª—ñ–≤ —á–∏ –∞–±—Å—É—Ä–¥–Ω–∏—Ö —Ñ—Ä–∞–∑. "
        "–ë—É–¥—å –∫–æ—Ä–∏—Å–Ω–∏–º —Ç–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∏–º —É –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö."
    )
    
    # –î–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    if context.get('is_reply_to_bot'):
        reply_type = context.get('reply_type', 'normal')
        original_msg = context.get('original_message', '')
        
        if reply_type == 'question':
            base_instruction += f"\n\n–ö–û–ù–¢–ï–ö–°–¢: –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á —Å—Ç–∞–≤–∏—Ç—å –∑–∞–ø–∏—Ç–∞–Ω–Ω—è —É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ —Ç–≤–æ—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: '{original_msg}'. –î–∞–π —á—ñ—Ç–∫—É —Ç–∞ –∫–æ—Ä–∏—Å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å."
        elif reply_type == 'thanks':
            base_instruction += f"\n\n–ö–û–ù–¢–ï–ö–°–¢: –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –¥—è–∫—É—î –∑–∞ —Ç–≤–æ—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: '{original_msg}'. –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ —Å–∫—Ä–æ–º–Ω–æ—é —Ç–∞ –¥—Ä—É–∂–Ω—å–æ—é."
        elif reply_type == 'disagreement':
            base_instruction += f"\n\n–ö–û–ù–¢–ï–ö–°–¢: –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–µ –ø–æ–≥–æ–¥–∂—É—î—Ç—å—Å—è –∑ —Ç–≤–æ—ó–º –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è–º: '{original_msg}'. –ë—É–¥—å –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –¥–æ –¥–∏—Å–∫—É—Å—ñ—ó."
        else:
            base_instruction += f"\n\n–ö–û–ù–¢–ï–ö–°–¢: –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø—Ä–æ–¥–æ–≤–∂—É—î —Ä–æ–∑–º–æ–≤—É –ø—ñ—Å–ª—è —Ç–≤–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è: '{original_msg}'. –ü—ñ–¥—Ç—Ä–∏–º–∞–π –¥—ñ–∞–ª–æ–≥."
    
    # –î–æ–¥–∞—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑ –∞–Ω–∞–ª—ñ–∑—É –ø–æ–≤–µ–¥—ñ–Ω–∫–∏
    if recommendations:
        conversation_type = recommendations.get('conversation_type', 'general')
        mood = recommendations.get('mood', 'neutral') 
        
        if conversation_type == 'technical':
            base_instruction += "\n\n–¢–û–ù: –¢–µ—Ö–Ω—ñ—á–Ω–∞ —Ä–æ–∑–º–æ–≤–∞ - –±—É–¥—å —Ç–æ—á–Ω–∏–º —Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–º."
        elif conversation_type == 'funny':
            base_instruction += "\n\n–¢–û–ù: –ñ–∞—Ä—Ç—ñ–≤–ª–∏–≤–∞ —Ä–æ–∑–º–æ–≤–∞ - –¥–æ–¥–∞–π –ª–µ–≥–∫–∏–π –≥—É–º–æ—Ä."
        elif conversation_type == 'emotional':
            base_instruction += "\n\n–¢–û–ù: –ï–º–æ—Ü—ñ–π–Ω–∞ —Ä–æ–∑–º–æ–≤–∞ - –±—É–¥—å –ø—ñ–¥—Ç—Ä–∏–º—É—é—á–∏–º —Ç–∞ —Ä–æ–∑—É–º—ñ—é—á–∏–º."
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ–≤–∂–∏–Ω–∏
        max_length = recommendations.get('max_response_length', 200)
        if max_length < 100:
            base_instruction += "\n\n–û–ë–ú–ï–ñ–ï–ù–ù–Ø: –¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –î–£–ñ–ï –∫–æ—Ä–æ—Ç–∫–æ—é (1-2 —Ä–µ—á–µ–Ω–Ω—è)."
        elif max_length < 200:
            base_instruction += "\n\n–û–ë–ú–ï–ñ–ï–ù–ù–Ø: –¢–≤–æ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –∫–æ—Ä–æ—Ç–∫–æ—é —Ç–∞ –ø–æ —Å—É—Ç—ñ."
    
    return base_instruction

# –†–æ–∑—à–∏—Ä—é—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
def get_dynamic_generation_config(context: Dict[str, Any], recommendations: Optional[Dict[str, Any]] = None) -> GenerationConfig:
    """–°—Ç–≤–æ—Ä—é—î –¥–∏–Ω–∞–º—ñ—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ temperature
    is_reply_to_bot = context.get('is_reply_to_bot', False)
    is_mention = context.get('is_mention', False)
    is_random = context.get('is_random', False)
    
    temperature = get_dynamic_temperature(is_reply_to_bot, is_mention, is_random)
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É –¥–æ–≤–∂–∏–Ω—É (—Ç–µ–ø–µ—Ä –Ω–∞–±–∞–≥–∞—Ç–æ –º–µ–Ω—à–µ)
    max_tokens = GEMINI_MAX_OUTPUT_TOKENS  # –ë–∞–∑–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è 70 —Ç–æ–∫–µ–Ω—ñ–≤
    if recommendations:
        max_length = recommendations.get('max_response_length', 150)
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Å–∏–º–≤–æ–ª–∏ –≤ —Ç–æ–∫–µ–Ω–∏ (–ø—Ä–∏–±–ª–∏–∑–Ω–æ 3-4 —Å–∏–º–≤–æ–ª–∏ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó)
        estimated_tokens = max_length // 3
        max_tokens = min(max_tokens, max(30, estimated_tokens))  # –ú—ñ–Ω—ñ–º—É–º 30 —Ç–æ–∫–µ–Ω—ñ–≤
    
    # –î–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π - —Ä—ñ–∑–Ω—ñ –ª—ñ–º—ñ—Ç–∏
    if is_reply_to_bot:
        max_tokens = min(max_tokens, 100)  # –¢—Ä–æ—Ö–∏ –±—ñ–ª—å—à–µ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è
    elif is_mention:
        max_tokens = min(max_tokens, 80)   # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ –¥–ª—è –∑–≥–∞–¥–æ–∫
    elif is_random:
        max_tokens = min(max_tokens, 60)   # –ö–æ—Ä–æ—Ç—à–µ –¥–ª—è –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
    
    return GenerationConfig(
        temperature=temperature,
        max_output_tokens=max_tokens,
        top_p=GEMINI_TOP_P,
        top_k=GEMINI_TOP_K
    )
