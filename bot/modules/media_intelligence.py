"""
–ú–æ–¥—É–ª—å Media Intelligence –¥–ª—è —Ä–æ–∑—É–º–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –º–µ–¥—ñ–∞
–ê–Ω–∞–ª—ñ–∑—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ —Ç–∞ —Å—Ç—ñ–∫–µ—Ä–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º AI
"""

import asyncio
import logging
import hashlib
import json
import os
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path

import aiofiles
import aiohttp
# from PIL import Image  # –û–ø—Ü—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
import io
import base64

logger = logging.getLogger(__name__)


@dataclass
class MediaAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É –º–µ–¥—ñ–∞"""
    media_id: str
    media_type: str
    description: str
    tags: List[str] = field(default_factory=list)
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    file_size: Optional[int] = None
    dimensions: Optional[Tuple[int, int]] = None
    duration: Optional[float] = None


@dataclass
class MediaCache:
    """–ö–µ—à –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É –º–µ–¥—ñ–∞"""
    cache: Dict[str, MediaAnalysis] = field(default_factory=dict)
    max_size: int = 1000
    cache_file: str = "data/media_cache.json"


class MediaIntelligence:
    """–†–æ–∑—É–º–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –º–µ–¥—ñ–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
    
    def __init__(self, 
                 gemini_api_key: Optional[str] = None,
                 cache_dir: str = "data",
                 max_cache_size: int = 1000,
                 enable_ai_analysis: bool = True):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Media Intelligence
        
        Args:
            gemini_api_key: API –∫–ª—é—á –¥–ª—è Gemini (–Ω–µ–æ–±–æ–≤'—è–∑–∫–æ–≤–æ)
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∫–µ—à—É
            max_cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∫–µ—à—É
            enable_ai_analysis: –£–≤—ñ–º–∫–Ω—É—Ç–∏ AI –∞–Ω–∞–ª—ñ–∑
        """
        self.gemini_api_key = gemini_api_key
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.enable_ai_analysis = enable_ai_analysis
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–µ—à—É
        self.cache = MediaCache(
            max_size=max_cache_size,
            cache_file=str(self.cache_dir / "media_cache.json")
        )
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É
        asyncio.create_task(self._load_cache())
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'analyzed_media': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'ai_calls': 0,
            'errors': 0
        }
        
        # –®–≤–∏–¥–∫—ñ –æ–ø–∏—Å–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –º–µ–¥—ñ–∞
        self.quick_descriptions = {
            'sticker': [
                "üòÑ –í–µ—Å–µ–ª–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "üò¢ –°—É–º–Ω–∏–π —Å—Ç—ñ–∫–µ—Ä", 
                "üòç –ú–∏–ª–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "ü§î –ó–∞–¥—É–º–ª–∏–≤–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "üò° –°–µ—Ä–¥–∏—Ç–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "üéâ –°–≤—è—Ç–∫–æ–≤–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "‚ù§Ô∏è –†–æ–º–∞–Ω—Ç–∏—á–Ω–∏–π —Å—Ç—ñ–∫–µ—Ä",
                "üòÇ –°–º—ñ—à–Ω–∏–π —Å—Ç—ñ–∫–µ—Ä"
            ],
            'photo': [
                "üì∑ –§–æ—Ç–æ–≥—Ä–∞—Ñ—ñ—è",
                "üåÖ –ö—Ä–∞—î–≤–∏–¥",
                "üë• –ì—Ä—É–ø–æ–≤–µ —Ñ–æ—Ç–æ",
                "üçï –á–∂–∞",
                "üè† –Ü–Ω—Ç–µ—Ä'—î—Ä",
                "üê± –¢–≤–∞—Ä–∏–Ω–∞",
                "üöó –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç",
                "üíª –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó"
            ],
            'voice': [
                "üé§ –ì–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è",
                "üéµ –ú—É–∑–∏–∫–∞",
                "üìª –ê—É–¥—ñ–æ –∑–∞–ø–∏—Å",
                "üó£Ô∏è –†–æ–∑–º–æ–≤–∞",
                "üò¥ –¢–∏—Ö–∏–π –≥–æ–ª–æ—Å",
                "üòÑ –í–µ—Å–µ–ª–∏–π –≥–æ–ª–æ—Å",
                "üò† –°–µ—Ä–¥–∏—Ç–∏–π –≥–æ–ª–æ—Å"
            ],
            'video': [
                "üé¨ –í—ñ–¥–µ–æ",
                "üéÆ –ì–µ–π–º–ø–ª–µ–π", 
                "üé≠ –†–æ–∑–≤–∞–≥–∏",
                "üìö –ù–∞–≤—á–∞–ª—å–Ω–µ –≤—ñ–¥–µ–æ",
                "üéµ –ú—É–∑–∏—á–Ω–µ –≤—ñ–¥–µ–æ",
                "üèÉ –°–ø–æ—Ä—Ç",
                "üç≥ –ö—É–ª—ñ–Ω–∞—Ä—ñ—è"
            ]
        }

    async def _load_cache(self) -> None:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É –∑ —Ñ–∞–π–ª—É"""
        try:
            if os.path.exists(self.cache.cache_file):
                async with aiofiles.open(self.cache.cache_file, 'r', encoding='utf-8') as f:
                    data = await f.read()
                    cache_data = json.loads(data)
                    
                    # –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–µ—à—É
                    for media_id, analysis_data in cache_data.items():
                        analysis = MediaAnalysis(
                            media_id=analysis_data['media_id'],
                            media_type=analysis_data['media_type'],
                            description=analysis_data['description'],
                            tags=analysis_data.get('tags', []),
                            confidence=analysis_data.get('confidence', 0.0),
                            timestamp=datetime.fromisoformat(analysis_data['timestamp']),
                            file_size=analysis_data.get('file_size'),
                            dimensions=tuple(analysis_data['dimensions']) if analysis_data.get('dimensions') else None,
                            duration=analysis_data.get('duration')
                        )
                        self.cache.cache[media_id] = analysis
                        
                logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.cache.cache)} –∑–∞–ø–∏—Å—ñ–≤ –∫–µ—à—É –º–µ–¥—ñ–∞")
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É –º–µ–¥—ñ–∞: {e}")

    async def _save_cache(self) -> None:
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–µ—à—É —É —Ñ–∞–π–ª"""
        try:
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–µ—à—É –≤ JSON-—Å—É–º—ñ—Å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
            cache_data = {}
            for media_id, analysis in self.cache.cache.items():
                cache_data[media_id] = {
                    'media_id': analysis.media_id,
                    'media_type': analysis.media_type,
                    'description': analysis.description,
                    'tags': analysis.tags,
                    'confidence': analysis.confidence,
                    'timestamp': analysis.timestamp.isoformat(),
                    'file_size': analysis.file_size,
                    'dimensions': list(analysis.dimensions) if analysis.dimensions else None,
                    'duration': analysis.duration
                }
            
            async with aiofiles.open(self.cache.cache_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(cache_data, ensure_ascii=False, indent=2))
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–µ—à—É –º–µ–¥—ñ–∞: {e}")

    def _generate_media_id(self, file_id: str, file_unique_id: Optional[str] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –º–µ–¥—ñ–∞"""
        content = f"{file_id}_{file_unique_id or ''}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_quick_description(self, media_type: str) -> str:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ–≥–æ –æ–ø–∏—Å—É –¥–ª—è –º–µ–¥—ñ–∞"""
        import random
        descriptions = self.quick_descriptions.get(media_type, ["–ú–µ–¥—ñ–∞ —Ñ–∞–π–ª"])
        return random.choice(descriptions)

    async def analyze_media(self, 
                          file_id: str,
                          media_type: str,
                          file_unique_id: Optional[str] = None,
                          file_size: Optional[int] = None,
                          use_ai: Optional[bool] = None) -> MediaAnalysis:
        """
        –ê–Ω–∞–ª—ñ–∑ –º–µ–¥—ñ–∞ —Ñ–∞–π–ª—É
        
        Args:
            file_id: ID —Ñ–∞–π–ª—É –≤ Telegram
            media_type: –¢–∏–ø –º–µ–¥—ñ–∞ (photo, sticker, voice, video)
            file_unique_id: –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π ID —Ñ–∞–π–ª—É
            file_size: –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É
            use_ai: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ AI –∞–Ω–∞–ª—ñ–∑ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å)
            
        Returns:
            MediaAnalysis: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
        """
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID –º–µ–¥—ñ–∞
            media_id = self._generate_media_id(file_id, file_unique_id)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
            if media_id in self.cache.cache:
                self.stats['cache_hits'] += 1
                return self.cache.cache[media_id]
            
            self.stats['cache_misses'] += 1
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —á–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ AI
            use_ai_analysis = use_ai if use_ai is not None else self.enable_ai_analysis
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
            analysis = MediaAnalysis(
                media_id=media_id,
                media_type=media_type,
                description=self._get_quick_description(media_type),
                file_size=file_size,
                confidence=0.5  # –ë–∞–∑–æ–≤–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –¥–ª—è —à–≤–∏–¥–∫–∏—Ö –æ–ø–∏—Å—ñ–≤
            )
            
            # AI –∞–Ω–∞–ª—ñ–∑ (—è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–∏–π —ñ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
            if use_ai_analysis and self.gemini_api_key:
                try:
                    ai_analysis = await self._analyze_with_ai(file_id, media_type)
                    if ai_analysis:
                        analysis.description = ai_analysis.get('description', analysis.description)
                        analysis.tags = ai_analysis.get('tags', [])
                        analysis.confidence = ai_analysis.get('confidence', 0.8)
                        self.stats['ai_calls'] += 1
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ AI –∞–Ω–∞–ª—ñ–∑—É –º–µ–¥—ñ–∞: {e}")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à
            await self._cache_analysis(media_id, analysis)
            
            self.stats['analyzed_media'] += 1
            return analysis
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É –º–µ–¥—ñ–∞ {file_id}: {e}")
            self.stats['errors'] += 1
            
            # –ü–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —É –≤–∏–ø–∞–¥–∫—É –ø–æ–º–∏–ª–∫–∏
            return MediaAnalysis(
                media_id=self._generate_media_id(file_id, file_unique_id),
                media_type=media_type,
                description=f"–ú–µ–¥—ñ–∞ —Ñ–∞–π–ª ({media_type})",
                confidence=0.1
            )

    async def _analyze_with_ai(self, file_id: str, media_type: str) -> Optional[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª—ñ–∑ –º–µ–¥—ñ–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º AI (Gemini)
        
        Args:
            file_id: ID —Ñ–∞–π–ª—É
            media_type: –¢–∏–ø –º–µ–¥—ñ–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç AI –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ None
        """
        if not self.gemini_api_key:
            return None
            
        try:
            # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—É –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–∏–ø—É –º–µ–¥—ñ–∞
            prompts = {
                'photo': "–û–ø–∏—à–∏ —Ü–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –æ–¥–Ω–∏–º —Ä–µ—á–µ–Ω–Ω—è–º —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –ë—É–¥—å —Å—Ç–∏—Å–ª–∏–º —Ç–∞ —Ç–æ—á–Ω–∏–º.",
                'sticker': "–û–ø–∏—à–∏ –µ–º–æ—Ü—ñ—é –∞–±–æ –∑–Ω–∞—á–µ–Ω–Ω—è —Ü—å–æ–≥–æ —Å—Ç—ñ–∫–µ—Ä–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –∞–±–æ —Ñ—Ä–∞–∑–æ—é.",
                'voice': "–¶–µ –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –û–ø–∏—à–∏ –º–æ–∂–ª–∏–≤–∏–π –Ω–∞—Å—Ç—Ä—ñ–π –∞–±–æ —Ç–æ–Ω —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.",
                'video': "–û–ø–∏—à–∏ –∑–º—ñ—Å—Ç —Ü—å–æ–≥–æ –≤—ñ–¥–µ–æ –æ–¥–Ω–∏–º —Ä–µ—á–µ–Ω–Ω—è–º —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é."
            }
            
            prompt = prompts.get(media_type, "–û–ø–∏—à–∏ —Ü–µ–π –º–µ–¥—ñ–∞ —Ñ–∞–π–ª —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.")
            
            # –¢—É—Ç –±–∏ –±—É–≤ –≤–∏–∫–ª–∏–∫ –¥–æ Gemini API
            # –ü–æ–∫–∏ —â–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –∑–∞–≥–ª—É—à–∫—É
            return {
                'description': f"AI –∞–Ω–∞–ª—ñ–∑: {media_type}",
                'tags': [media_type, 'ai_analyzed'],
                'confidence': 0.8
            }
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ AI –∞–Ω–∞–ª—ñ–∑—É: {e}")
            return None

    async def _cache_analysis(self, media_id: str, analysis: MediaAnalysis) -> None:
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É –≤ –∫–µ—à"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É –∫–µ—à—É
            if len(self.cache.cache) >= self.cache.max_size:
                await self._cleanup_cache()
            
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è –≤ –∫–µ—à
            self.cache.cache[media_id] = analysis
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–µ—à—É (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
            asyncio.create_task(self._save_cache())
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É: {e}")

    async def _cleanup_cache(self) -> None:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –∫–µ—à—É"""
        try:
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —á–∞—Å–æ–º (–Ω–∞–π—Å—Ç–∞—Ä—ñ—à—ñ –ø–µ—Ä—à—ñ)
            sorted_items = sorted(
                self.cache.cache.items(),
                key=lambda x: x[1].timestamp
            )
            
            # –í–∏–¥–∞–ª–µ–Ω–Ω—è 20% –Ω–∞–π—Å—Ç–∞—Ä—ñ—à–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
            items_to_remove = len(sorted_items) // 5
            for i in range(items_to_remove):
                media_id = sorted_items[i][0]
                del self.cache.cache[media_id]
                
            logger.info(f"–û—á–∏—â–µ–Ω–æ {items_to_remove} –∑–∞–ø–∏—Å—ñ–≤ –∑ –∫–µ—à—É –º–µ–¥—ñ–∞")
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É: {e}")

    async def get_media_summary(self, chat_id: int, limit: int = 100) -> Dict[str, Any]:
        """
        –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–µ–¥—ñ–∞ –¥–ª—è —á–∞—Ç—É
        
        Args:
            chat_id: ID —á–∞—Ç—É
            limit: –õ—ñ–º—ñ—Ç –∑–∞–ø–∏—Å—ñ–≤
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ–¥—ñ–∞
        """
        try:
            # –ê–Ω–∞–ª—ñ–∑ –∫–µ—à—É –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            media_types = {}
            recent_media = []
            
            # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞–ø–∏—Å—ñ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –±–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞—Å—å –ë–î)
            for analysis in self.cache.cache.values():
                media_type = analysis.media_type
                media_types[media_type] = media_types.get(media_type, 0) + 1
                
                if len(recent_media) < limit:
                    recent_media.append({
                        'type': analysis.media_type,
                        'description': analysis.description,
                        'timestamp': analysis.timestamp.isoformat(),
                        'confidence': analysis.confidence
                    })
            
            return {
                'total_analyzed': len(self.cache.cache),
                'media_types': media_types,
                'recent_media': recent_media[:limit],
                'stats': self.stats.copy()
            }
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–µ–¥—ñ–∞: {e}")
            return {
                'total_analyzed': 0,
                'media_types': {},
                'recent_media': [],
                'stats': self.stats.copy()
            }

    async def search_media(self, 
                          query: str,
                          media_type: Optional[str] = None,
                          limit: int = 50) -> List[MediaAnalysis]:
        """
        –ü–æ—à—É–∫ –º–µ–¥—ñ–∞ –∑–∞ –æ–ø–∏—Å–æ–º –∞–±–æ —Ç–µ–≥–∞–º–∏
        
        Args:
            query: –ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç
            media_type: –§—ñ–ª—å—Ç—Ä –∑–∞ —Ç–∏–ø–æ–º –º–µ–¥—ñ–∞
            limit: –õ—ñ–º—ñ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –º–µ–¥—ñ–∞
        """
        try:
            results = []
            query_lower = query.lower()
            
            for analysis in self.cache.cache.values():
                # –§—ñ–ª—å—Ç—Ä –∑–∞ —Ç–∏–ø–æ–º
                if media_type and analysis.media_type != media_type:
                    continue
                
                # –ü–æ—à—É–∫ –≤ –æ–ø–∏—Å—ñ —Ç–∞ —Ç–µ–≥–∞—Ö
                if (query_lower in analysis.description.lower() or 
                    any(query_lower in tag.lower() for tag in analysis.tags)):
                    results.append(analysis)
                    
                    if len(results) >= limit:
                        break
            
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é (–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é)
            results.sort(key=lambda x: x.confidence, reverse=True)
            
            return results
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –º–µ–¥—ñ–∞: {e}")
            return []

    async def get_health_status(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –∑–¥–æ—Ä–æ–≤'—è –º–æ–¥—É–ª—è"""
        try:
            cache_size = len(self.cache.cache)
            cache_health = "healthy" if cache_size < self.cache.max_size * 0.8 else "warning"
            
            return {
                'status': 'healthy',
                'cache_size': cache_size,
                'cache_max_size': self.cache.max_size,
                'cache_health': cache_health,
                'ai_enabled': self.enable_ai_analysis,
                'stats': self.stats.copy(),
                'last_check': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è MediaIntelligence: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'last_check': datetime.now().isoformat()
            }

    async def cleanup_old_data(self, days: int = 30) -> None:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            removed_count = 0
            
            # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
            media_ids_to_remove = []
            for media_id, analysis in self.cache.cache.items():
                if analysis.timestamp < cutoff_date:
                    media_ids_to_remove.append(media_id)
            
            for media_id in media_ids_to_remove:
                del self.cache.cache[media_id]
                removed_count += 1
            
            if removed_count > 0:
                await self._save_cache()
                logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –º–µ–¥—ñ–∞ –∞–Ω–∞–ª—ñ–∑—É")
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö MediaIntelligence: {e}")
