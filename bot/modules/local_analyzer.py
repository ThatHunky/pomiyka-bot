# –õ–æ–∫–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–æ–∑–º–æ–≤ - –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è i5-6500, 16GB RAM
import json
import sqlite3
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
import re
import hashlib

try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("sentence-transformers –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –ë—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ fallback –∞–Ω–∞–ª—ñ–∑.")

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    logging.warning("spaCy –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –î–µ—è–∫—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –±—É–¥—É—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ.")

from bot.bot_config import *

class LocalAnalyzer:
    """–õ–æ–∫–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä —Ä–æ–∑–º–æ–≤ –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é –¥–ª—è –æ–±–º–µ–∂–µ–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤"""
    
    def __init__(self, db_path: str = "data/analysis_cache.db"):
        self.db_path = db_path
        self.model = None
        self.nlp = None
        self.embedding_cache = {}
        self.analysis_cache = {}
        self.batch_size = 5  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è i5-6500
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
        self._init_database()
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        self._load_models()
        
        # –ï–º–æ—Ü—ñ–π–Ω—ñ —Å–ª–æ–≤–Ω–∏–∫–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        self._init_emotion_dictionaries()
        
        # –¢–µ–º–∞—Ç–∏—á–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        self._init_topic_keywords()

    def _init_database(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –¢–∞–±–ª–∏—Ü—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—å
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS embeddings_cache (
                    text_hash TEXT PRIMARY KEY,
                    embedding BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü—è –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analysis_cache (
                    text_hash TEXT PRIMARY KEY,
                    emotion TEXT,
                    topic TEXT,
                    confidence REAL,
                    keywords TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—ñ–≤ —Ä–æ–∑–º–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS conversation_summaries (
                    chat_id INTEGER,
                    time_period TEXT,
                    summary TEXT,
                    dominant_emotion TEXT,
                    main_topics TEXT,
                    participants_count INTEGER,
                    message_count INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (chat_id, time_period)
                )
            ''')
            
            conn.commit()
            conn.close()
            logging.info("–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: {e}")

    def _load_models(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫"""
        try:
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                # –õ–µ–≥–∫–∞ –±–∞–≥–∞—Ç–æ–º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è CPU
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logging.info("Sentence Transformer –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            
            if SPACY_AVAILABLE:
                try:
                    # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–¥–µ–ª—å
                    self.nlp = spacy.load("uk_core_news_sm")
                    logging.info("spaCy —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
                except OSError:
                    # Fallback –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –º–æ–¥–µ–ª—å
                    try:
                        self.nlp = spacy.load("en_core_web_sm")
                        logging.info("spaCy –∞–Ω–≥–ª—ñ–π—Å—å–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —è–∫ fallback")
                    except OSError:
                        logging.warning("–ñ–æ–¥–Ω–∞ spaCy –º–æ–¥–µ–ª—å –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
                        
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π: {e}")

    def _init_emotion_dictionaries(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ –µ–º–æ—Ü—ñ–π –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É"""
        self.emotion_words = {
            "—Ä–∞–¥—ñ—Å—Ç—å": [
                "—Ä–∞–¥–∏–π", "—Ä–∞–¥—ñ—Å—Ç—å", "—â–∞—Å–ª–∏–≤–∏–π", "—â–∞—Å—Ç—è", "–≤–µ—Å–µ–ª–∏–π", "–≤–µ—Å–µ–ª–æ", "–∫—Ä—É—Ç–æ", "–∫–ª–∞—Å–Ω–æ", 
                "—Å—É–ø–µ—Ä", "—á—É–¥–æ–≤–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", "–≤—ñ–¥–º—ñ–Ω–Ω–æ", "—É—Ä–∞", "–≤–∞—É", "üòä", "üòÉ", "üòÑ", 
                "üòÅ", "üòÜ", "üòÇ", "ü§£", "üòç", "ü•∞", "‚ù§Ô∏è", "üëç", "üî•", "üéâ", "ü•≥"
            ],
            "—Å—É–º": [
                "—Å—É–º–Ω–æ", "—Å—É–º", "–≥—Ä—É—Å—Ç–Ω–æ", "–ø–ª–∞—á—É", "–∑–∞—Å–º—É—á–µ–Ω–∏–π", "–≥—ñ—Ä–∫–æ", "–∂–∞–ª—å", "–∂–∞–ª–∫–æ",
                "–ø–µ—á–∞–ª—å–Ω–æ", "–º–µ–ª–∞–Ω—Ö–æ–ª—ñ—è", "üò¢", "üò≠", "üòî", "üòû", "üòü", "üò•", "üò™", "üíî", "üòø"
            ],
            "–∑–ª—ñ—Å—Ç—å": [
                "–∑–ª–∏–π", "–∑–ª—ñ—Å—Ç—å", "–±—ñ—Å–Ω–∏–π", "—Ä–æ–∑–¥—Ä–∞—Ç–æ–≤–∞–Ω–∏–π", "–¥—Ä–∞—Ç—É—î", "–Ω–µ—Ä–≤—É—î", "–ª—é—Ç—å", "–≥–Ω—ñ–≤",
                "–æ–±—É—Ä–µ–Ω–∏–π", "–ª–∞–π–Ω–æ", "—Ñ—ñ–≥–Ω—è", "–¥—É—Ä–Ω–∏—Ü—è", "üò°", "üò†", "ü§¨", "üëé", "üí©", "üñï"
            ],
            "—Å—Ç—Ä–∞—Ö": [
                "–±–æ—é—Å—å", "—Å—Ç—Ä–∞—Ö", "—Å—Ç—Ä–∞—à–Ω–æ", "–ª—è–∫–∞—î", "—Ç—Ä–∏–≤–æ–≥–∞", "—Ö–≤–∏–ª—é–≤–∞–Ω–Ω—è", "–Ω–µ—Ä–≤—É—é", 
                "–ø–µ—Ä–µ–∂–∏–≤–∞—é", "–ø–∞–Ω—ñ–∫—É—é", "üò®", "üò∞", "üò±", "üòß", "üò¶", "üòü"
            ],
            "–∑–¥–∏–≤—É–≤–∞–Ω–Ω—è": [
                "–æ–≥–æ", "–≤–∞—É", "–Ω–µ–π–º–æ–≤—ñ—Ä–Ω–æ", "–¥–∏–≤–æ–≤–∏–∂–Ω–æ", "—à–æ–∫", "—à–æ–∫—É—î", "–≤—Ä–∞–∂–∞—î", "–∫–∞–ø–µ—Ü—å",
                "–Ω–µ –º–æ–∂–µ –±—É—Ç–∏", "üòÆ", "üòØ", "üò≤", "ü§Ø", "üò±", "üôÑ"
            ],
            "–≤—ñ–¥—Ä–∞–∑–∞": [
                "—Ñ—É", "–æ–≥–∏–¥–Ω–æ", "–±—Ä–∏–¥–∫–æ", "–ø—Ä–æ—Ç–∏–≤–Ω–æ", "–Ω–µ–Ω–∞–≤–∏–¥–∂—É", "–≥–∏–¥–æ—Ç–∞", "–º–µ—Ä–∑–æ—Ç–∞",
                "ü§¢", "ü§Æ", "üò∑", "ü§ß", "üòñ", "üò£"
            ]
        }

    def _init_topic_keywords(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º"""
        self.topic_keywords = {
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó": [
                "–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–∞", "–ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è", "–∞–ª–≥–æ—Ä–∏—Ç–º", "—Ñ—É–Ω–∫—Ü—ñ—è", "–∑–º—ñ–Ω–Ω–∞", "–∫–ª–∞—Å",
                "python", "javascript", "java", "react", "node", "git", "github", "api", "json",
                "—Å–µ—Ä–≤–µ—Ä", "–±–∞–∑–∞", "–¥–∞–Ω–∏—Ö", "sql", "mongodb", "docker", "linux", "windows"
            ],
            "—Ä–æ–±–æ—Ç–∞_–Ω–∞–≤—á–∞–Ω–Ω—è": [
                "—Ä–æ–±–æ—Ç–∞", "–æ—Ñ—ñ—Å", "–ø—Ä–æ–µ–∫—Ç", "–∑–∞–≤–¥–∞–Ω–Ω—è", "–¥–µ–¥–ª–∞–π–Ω", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—è", "–∑–≤—ñ—Ç",
                "—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–Ω–∞–≤—á–∞–Ω–Ω—è", "–µ–∫–∑–∞–º–µ–Ω", "—Å–µ—Å—ñ—è", "–¥–∏–ø–ª–æ–º", "–∫—É—Ä—Å–æ–≤–∞", "–ª–µ–∫—Ü—ñ—è"
            ],
            "–ø–æ–≤—Å—è–∫–¥–µ–Ω–Ω–µ": [
                "—ó–∂–∞", "–≥–æ—Ç—É—é", "–æ–±—ñ–¥", "–≤–µ—á–µ—Ä—è", "—Å–Ω—ñ–¥–∞–Ω–æ–∫", "–∫—É—Ö–Ω—è", "—Ä–µ—Ü–µ–ø—Ç", "—Å–º–∞—á–Ω–æ",
                "–¥—ñ–º", "–∫–≤–∞—Ä—Ç–∏—Ä–∞", "–ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è", "–ø–æ–∫—É–ø–∫–∏", "–º–∞–≥–∞–∑–∏–Ω", "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "–¥–æ—Ä–æ–≥–∞"
            ],
            "—Ä–æ–∑–≤–∞–≥–∏": [
                "—Ñ—ñ–ª—å–º", "—Å–µ—Ä—ñ–∞–ª", "–º—É–∑–∏–∫–∞", "–ø—ñ—Å–Ω—è", "–≥—Ä–∞", "—ñ–≥—Ä–∞", "—Å–ø–æ—Ä—Ç", "—Ñ—É—Ç–±–æ–ª", "–∫—ñ–Ω–æ",
                "—Ç–µ–∞—Ç—Ä", "–∫–æ–Ω—Ü–µ—Ä—Ç", "–∫–Ω–∏–≥–∞", "—á–∏—Ç–∞—é", "–º–∞–Ω–¥—Ä—ñ–≤–∫–∞", "–ø–æ–¥–æ—Ä–æ–∂", "–≤—ñ–¥–ø—É—Å—Ç–∫–∞"
            ],
            "–ø–æ–≥–æ–¥–∞_–ø—Ä–∏—Ä–æ–¥–∞": [
                "–ø–æ–≥–æ–¥–∞", "–¥–æ—â", "—Å–æ–Ω—Ü–µ", "—Å–Ω—ñ–≥", "–≤—ñ—Ç–µ—Ä", "—Ç–µ–ø–ª–æ", "—Ö–æ–ª–æ–¥–Ω–æ", "–ª—ñ—Ç–æ", "–∑–∏–º–∞",
                "–≤–µ—Å–Ω–∞", "–æ—Å—ñ–Ω—å", "–ø—Ä–∏—Ä–æ–¥–∞", "–ª—ñ—Å", "–º–æ—Ä–µ", "–≥–æ—Ä–∏", "–ø–∞—Ä–∫"
            ],
            "–≤—ñ–¥–Ω–æ—Å–∏–Ω–∏": [
                "–ª—é–±–æ–≤", "—Å—Ç–æ—Å—É–Ω–∫–∏", "–¥—ñ–≤—á–∏–Ω–∞", "—Ö–ª–æ–ø–µ—Ü—å", "–¥—Ä—É–∂–±–∞", "–¥—Ä—É–∑—ñ", "—Å—ñ–º'—è", "–º–∞–º–∞",
                "—Ç–∞—Ç–æ", "—Ä–æ–¥–∏—á—ñ", "–æ–¥—Ä—É–∂–µ–Ω–Ω—è", "–≤–µ—Å—ñ–ª–ª—è", "–¥—ñ—Ç–∏", "–∫–æ—Ö–∞–Ω–Ω—è"
            ]
        }

    def get_text_hash(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä—É—î —Ö–µ—à –¥–ª—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get_cached_analysis(self, text: str) -> Optional[Dict[str, Any]]:
        """–û—Ç—Ä–∏–º—É—î –∑–∞–∫–µ—à–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É"""
        text_hash = self.get_text_hash(text)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT emotion, topic, confidence, keywords 
                FROM analysis_cache 
                WHERE text_hash = ? AND created_at > datetime('now', '-24 hours')
            ''', (text_hash,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return {
                    "emotion": result[0],
                    "topic": result[1],
                    "confidence": result[2],
                    "keywords": json.loads(result[3]) if result[3] else []
                }
                
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–µ—à—É: {e}")
            
        return None

    def cache_analysis(self, text: str, analysis: Dict[str, Any]):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É –≤ –∫–µ—à"""
        text_hash = self.get_text_hash(text)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO analysis_cache 
                (text_hash, emotion, topic, confidence, keywords)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                text_hash,
                analysis.get("emotion", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π"),
                analysis.get("topic", "–∑–∞–≥–∞–ª—å–Ω–µ"),
                analysis.get("confidence", 0.5),
                json.dumps(analysis.get("keywords", []))
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à: {e}")

    def analyze_emotion_fast(self, text: str) -> Tuple[str, float]:
        """–®–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ –µ–º–æ—Ü—ñ–π –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        text_lower = text.lower()
        emotion_scores = defaultdict(float)
        
        for emotion, words in self.emotion_words.items():
            score = 0
            for word in words:
                if word in text_lower:
                    # –ï–º–æ–¥–∑—ñ –º–∞—é—Ç—å –±—ñ–ª—å—à—É –≤–∞–≥—É
                    weight = 2 if any(ord(char) > 127 for char in word) else 1
                    score += weight
            
            if score > 0:
                emotion_scores[emotion] = score / len(words)
        
        if emotion_scores:
            dominant_emotion = max(emotion_scores, key=emotion_scores.get)
            confidence = min(emotion_scores[dominant_emotion] * 2, 1.0)
            return dominant_emotion, confidence
        
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", 0.3

    def analyze_topic_fast(self, text: str) -> Tuple[str, float]:
        """–®–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ —Ç–µ–º–∏ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        text_lower = text.lower()
        topic_scores = defaultdict(float)
        
        for topic, keywords in self.topic_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                topic_scores[topic] = score / len(keywords)
        
        if topic_scores:
            dominant_topic = max(topic_scores, key=topic_scores.get)
            confidence = min(topic_scores[dominant_topic] * 3, 1.0)
            return dominant_topic, confidence
        
        return "–∑–∞–≥–∞–ª—å–Ω–µ", 0.3

    def extract_keywords_simple(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤"""
        # –í–∏–¥–∞–ª—è—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é —Ç–∞ —Ä–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ë—ñ—ó]{3,}\b', text.lower())
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            "—â–æ", "—è–∫–∏–π", "—è–∫–∞", "—è–∫–µ", "—è–∫–∏–π", "—Ü–µ–π", "—Ü—è", "—Ü–µ", "—Ç–æ–π", "—Ç–∞", "—Ç–µ",
            "–º–µ–Ω–µ", "—Ç–µ–±–µ", "–π–æ–≥–æ", "–Ω–µ—ó", "–Ω–∞—Å", "–≤–∞—Å", "–Ω–∏—Ö", "–¥–ª—è", "–ø—Ä–æ", "–ø—Ä–∏",
            "–Ω–∞–¥", "–ø—ñ–¥", "–º—ñ–∂", "–±–µ–∑", "—á–µ—Ä–µ–∑", "–ø—ñ—Å–ª—è", "–ø–µ—Ä–µ–¥", "–≤–∂–µ", "—â–µ", "–¥—É–∂–µ",
            "—Ç–æ–º—É", "—Ç—ñ–ª—å–∫–∏", "—Ç–∞–∫–æ–∂", "–º–æ–∂–µ", "–º–æ–∂–Ω–∞", "—Ç—Ä–µ–±–∞", "–±—É–¥–µ", "–±—É–ª–∞", "–±—É–ª–æ"
        }
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–∞–π—á–∞—Å—Ç—ñ—à—ñ
        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
        word_counts = Counter(filtered_words)
        
        return [word for word, count in word_counts.most_common(5)]

    async def analyze_message(self, text: str, use_cache: bool = True) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è"""
        if not text or len(text.strip()) < 3:
            return {
                "emotion": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π",
                "topic": "–∑–∞–≥–∞–ª—å–Ω–µ", 
                "confidence": 0.1,
                "keywords": [],
                "analysis_method": "minimal"
            }
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        if use_cache:
            cached = self.get_cached_analysis(text)
            if cached:
                cached["analysis_method"] = "cached"
                return cached
        
        # –®–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑
        emotion, emotion_conf = self.analyze_emotion_fast(text)
        topic, topic_conf = self.analyze_topic_fast(text)
        keywords = self.extract_keywords_simple(text)
        
        # –°–µ—Ä–µ–¥–Ω—è –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å
        avg_confidence = (emotion_conf + topic_conf) / 2
        
        analysis = {
            "emotion": emotion,
            "topic": topic,
            "confidence": avg_confidence,
            "keywords": keywords,
            "analysis_method": "fast_local"
        }
        
        # –Ø–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ —Ç–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –Ω–∏–∑—å–∫–∞ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—Ö
        if self.model and avg_confidence < 0.6:
            try:
                enhanced_analysis = await self._analyze_with_transformers(text, analysis)
                analysis.update(enhanced_analysis)
                analysis["analysis_method"] = "enhanced_local"
            except Exception as e:
                logging.warning(f"–ü–æ–º–∏–ª–∫–∞ enhanced –∞–Ω–∞–ª—ñ–∑—É: {e}")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
        if use_cache:
            self.cache_analysis(text, analysis)
        
        return analysis

    async def _analyze_with_transformers(self, text: str, base_analysis: Dict) -> Dict[str, Any]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º transformer –º–æ–¥–µ–ª–µ–π"""
        if not self.model:
            return {}
        
        try:
            # –ì–µ–Ω–µ—Ä—É—î–º–æ –µ–º–±–µ–¥–¥—ñ–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç—É
            embedding = self.model.encode([text])[0]
            
            # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ –µ—Ç–∞–ª–æ–Ω–Ω–∏–º–∏ –µ–º–±–µ–¥–¥—ñ–Ω–≥–∞–º–∏ –µ–º–æ—Ü—ñ–π (—Å–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)
            emotion_examples = {
                "—Ä–∞–¥—ñ—Å—Ç—å": "–Ø–∫ –∂–µ —è —â–∞—Å–ª–∏–≤–∏–π! –¶–µ –Ω–µ–π–º–æ–≤—ñ—Ä–Ω–æ –∫—Ä—É—Ç–æ!",
                "—Å—É–º": "–ú–µ–Ω—ñ —Ç–∞–∫ —Å—É–º–Ω–æ —ñ –≥—ñ—Ä–∫–æ –Ω–∞ –¥—É—à—ñ",
                "–∑–ª—ñ—Å—Ç—å": "–¶–µ –ø—Ä–æ—Å—Ç–æ –±—ñ—Å–∏—Ç—å! –Ø–∫–∞ —Ñ—ñ–≥–Ω—è!",
                "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π": "–°—å–æ–≥–æ–¥–Ω—ñ –∑–≤–∏—á–∞–π–Ω–∏–π –¥–µ–Ω—å, –Ω—ñ—á–æ–≥–æ –æ—Å–æ–±–ª–∏–≤–æ–≥–æ"
            }
            
            emotion_similarities = {}
            for emotion, example in emotion_examples.items():
                example_embedding = self.model.encode([example])[0]
                similarity = np.dot(embedding, example_embedding) / (
                    np.linalg.norm(embedding) * np.linalg.norm(example_embedding)
                )
                emotion_similarities[emotion] = similarity
            
            # –Ø–∫—â–æ –Ω–µ–π—Ä–æ–Ω–∫–∞ –¥–∞—î –∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –æ–Ω–æ–≤–ª—é—î–º–æ
            best_emotion = max(emotion_similarities, key=emotion_similarities.get)
            best_similarity = emotion_similarities[best_emotion]
            
            if best_similarity > 0.7 and best_similarity > base_analysis["confidence"]:
                return {
                    "emotion": best_emotion,
                    "confidence": min(best_similarity, 0.95)
                }
                
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ transformer –∞–Ω–∞–ª—ñ–∑—É: {e}")
        
        return {}

    async def analyze_conversation_batch(self, messages: List[Dict[str, Any]], chat_id: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ –ø–∞–∫–µ—Ç—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ä–æ–∑–º–æ–≤–∏"""
        if not messages:
            return {"summary": "–ü–æ—Ä–æ–∂–Ω—è —Ä–æ–∑–º–æ–≤–∞", "dominant_emotion": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", "main_topics": []}
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∫–æ–∂–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        analyses = []
        for message in messages[-self.batch_size:]:  # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ N –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
            text = message.get("text", "")
            if text:
                analysis = await self.analyze_message(text)
                analyses.append(analysis)
        
        if not analyses:
            return {"summary": "–ù–µ–º–∞—î —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å", "dominant_emotion": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π", "main_topics": []}
        
        # –ó–±–∏—Ä–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        emotions = [a["emotion"] for a in analyses]
        topics = [a["topic"] for a in analyses]
        all_keywords = []
        for a in analyses:
            all_keywords.extend(a["keywords"])
        
        # –î–æ–º—ñ–Ω—É—é—á–∞ –µ–º–æ—Ü—ñ—è
        emotion_counts = Counter(emotions)
        dominant_emotion = emotion_counts.most_common(1)[0][0] if emotion_counts else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π"
        
        # –û—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏
        topic_counts = Counter(topics)
        main_topics = [topic for topic, count in topic_counts.most_common(3)]
        
        # –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        keyword_counts = Counter(all_keywords)
        top_keywords = [kw for kw, count in keyword_counts.most_common(5)]
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å
        summary = self._generate_conversation_summary(messages, dominant_emotion, main_topics, top_keywords)
        
        result = {
            "summary": summary,
            "dominant_emotion": dominant_emotion,
            "main_topics": main_topics,
            "top_keywords": top_keywords,
            "message_count": len(messages),
            "analyzed_count": len(analyses),
            "emotion_distribution": dict(emotion_counts),
            "topic_distribution": dict(topic_counts)
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—ñ–¥—Å—É–º–æ–∫ –≤ –ë–î
        self._save_conversation_summary(chat_id, result)
        
        return result

    def _generate_conversation_summary(self, messages: List[Dict], emotion: str, topics: List[str], keywords: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å —Ä–æ–∑–º–æ–≤–∏"""
        participants = set()
        for msg in messages:
            user = msg.get("user", "")
            if user:
                participants.add(user)
        
        participant_count = len(participants)
        message_count = len(messages)
        
        # –ë–∞–∑–æ–≤–∏–π –æ–ø–∏—Å
        if message_count == 1:
            base = "–û–¥–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è"
        elif message_count < 5:
            base = f"–ö–æ—Ä–æ—Ç–∫–∞ —Ä–æ–∑–º–æ–≤–∞ ({message_count} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å)"
        else:
            base = f"–ê–∫—Ç–∏–≤–Ω–∞ —Ä–æ–∑–º–æ–≤–∞ ({message_count} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å)"
        
        # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —É—á–∞—Å–Ω–∏–∫—ñ–≤
        if participant_count > 1:
            base += f" –º—ñ–∂ {participant_count} —É—á–∞—Å–Ω–∏–∫–∞–º–∏"
        
        # –î–æ–¥–∞—î–º–æ –µ–º–æ—Ü—ñ–π–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        emotion_desc = {
            "—Ä–∞–¥—ñ—Å—Ç—å": "–∑ –≤–µ—Å–µ–ª–∏–º –Ω–∞—Å—Ç—Ä–æ—î–º",
            "—Å—É–º": "–∑ —Å—É–º–Ω–∏–º –≤—ñ–¥—Ç—ñ–Ω–∫–æ–º", 
            "–∑–ª—ñ—Å—Ç—å": "–∑ –Ω–∞–ø—Ä—É–∂–µ–Ω–æ—é –∞—Ç–º–æ—Å—Ñ–µ—Ä–æ—é",
            "–∑–¥–∏–≤—É–≤–∞–Ω–Ω—è": "–∑ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏ –∑–¥–∏–≤—É–≤–∞–Ω–Ω—è",
            "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π": "–≤ —Å–ø–æ–∫—ñ–π–Ω–æ–º—É —Ç–æ–Ω—ñ"
        }
        base += f" {emotion_desc.get(emotion, '–≤ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–º—É —Ç–æ–Ω—ñ')}"
        
        # –î–æ–¥–∞—î–º–æ —Ç–µ–º–∞—Ç–∏–∫—É
        if topics and topics[0] != "–∑–∞–≥–∞–ª—å–Ω–µ":
            if len(topics) == 1:
                base += f". –¢–µ–º–∞: {topics[0]}"
            else:
                base += f". –¢–µ–º–∏: {', '.join(topics[:2])}"
        
        # –î–æ–¥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —è–∫—â–æ –≤–æ–Ω–∏ —Ü—ñ–∫–∞–≤—ñ
        interesting_keywords = [kw for kw in keywords if len(kw) > 4]
        if interesting_keywords:
            base += f". –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {', '.join(interesting_keywords[:3])}"
        
        return base

    def _save_conversation_summary(self, chat_id: int, summary_data: Dict[str, Any]):
        """–ó–±–µ—Ä—ñ–≥–∞—î –ø—ñ–¥—Å—É–º–æ–∫ —Ä–æ–∑–º–æ–≤–∏ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            time_period = datetime.now().strftime("%Y-%m-%d_%H")  # –ì–æ–¥–∏–Ω–∞ —è–∫ –ø–µ—Ä—ñ–æ–¥
            
            cursor.execute('''
                INSERT OR REPLACE INTO conversation_summaries
                (chat_id, time_period, summary, dominant_emotion, main_topics, 
                 participants_count, message_count)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                chat_id,
                time_period,
                summary_data["summary"],
                summary_data["dominant_emotion"],
                json.dumps(summary_data["main_topics"]),
                summary_data.get("participants_count", 1),
                summary_data["message_count"]
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É: {e}")

    def get_chat_context_summary(self, chat_id: int, hours: int = 24) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º—É—î –ø—ñ–¥—Å—É–º–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —á–∞—Ç—É –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ –≥–æ–¥–∏–Ω–∏"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø—ñ–¥—Å—É–º–∫–∏ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ –≥–æ–¥–∏–Ω–∏
            cutoff_time = (datetime.now() - timedelta(hours=hours)).strftime("%Y-%m-%d_%H")
            
            cursor.execute('''
                SELECT summary, dominant_emotion, main_topics, message_count
                FROM conversation_summaries
                WHERE chat_id = ? AND time_period >= ?
                ORDER BY time_period DESC
            ''', (chat_id, cutoff_time))
            
            results = cursor.fetchall()
            conn.close()
            
            if not results:
                return {
                    "status": "no_data",
                    "summary": "–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –ø—Ä–æ —Ä–æ–∑–º–æ–≤—É",
                    "dominant_emotions": [],
                    "main_topics": [],
                    "total_messages": 0
                }
            
            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∑—ñ–±—Ä–∞–Ω—ñ –¥–∞–Ω—ñ
            all_emotions = []
            all_topics = []
            total_messages = 0
            summaries = []
            
            for result in results:
                summary, emotion, topics_json, msg_count = result
                summaries.append(summary)
                all_emotions.append(emotion)
                
                try:
                    topics = json.loads(topics_json)
                    all_topics.extend(topics)
                except:
                    pass
                    
                total_messages += msg_count
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            emotion_counts = Counter(all_emotions)
            topic_counts = Counter(all_topics)
            
            return {
                "status": "success",
                "summary": f"–ó–∞ –æ—Å—Ç–∞–Ω–Ω—ñ {hours} –≥–æ–¥–∏–Ω: {total_messages} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å",
                "detailed_summaries": summaries[:3],  # –û—Å—Ç–∞–Ω–Ω—ñ 3 –ø—ñ–¥—Å—É–º–∫–∏
                "dominant_emotions": [e for e, c in emotion_counts.most_common(3)],
                "main_topics": [t for t, c in topic_counts.most_common(5)],
                "total_messages": total_messages,
                "analysis_periods": len(results)
            }
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {e}")
            return {
                "status": "error",
                "summary": "–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É",
                "dominant_emotions": [],
                "main_topics": [],
                "total_messages": 0
            }

    def cleanup_old_data(self, days: int = 7):
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –º—ñ—Å—Ü—è"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cutoff_date = datetime.now() - timedelta(days=days)
            
            # –û—á–∏—â–∞—î–º–æ —Å—Ç–∞—Ä—ñ –∫–µ—à—ñ
            cursor.execute('DELETE FROM embeddings_cache WHERE created_at < ?', (cutoff_date,))
            cursor.execute('DELETE FROM analysis_cache WHERE created_at < ?', (cutoff_date,))
            
            # –û—á–∏—â–∞—î–º–æ —Å—Ç–∞—Ä—ñ –ø—ñ–¥—Å—É–º–∫–∏ (–∑–∞–ª–∏—à–∞—î–º–æ –±—ñ–ª—å—à–µ —á–∞—Å—É)
            old_cutoff = datetime.now() - timedelta(days=days*2)
            cursor.execute('DELETE FROM conversation_summaries WHERE created_at < ?', (old_cutoff,))
            
            conn.commit()
            conn.close()
            
            logging.info(f"–í–∏–¥–∞–ª–µ–Ω–æ –¥–∞–Ω—ñ —Å—Ç–∞—Ä—à–µ {days} –¥–Ω—ñ–≤")
            
        except Exception as e:
            logging.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")

    def analyze_text_sentiment(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–∏–π –∞–Ω–∞–ª—ñ–∑ –Ω–∞—Å—Ç—Ä–æ—é —Ç–µ–∫—Å—Ç—É."""
        # –®–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        positive_words = ["–¥–æ–±—Ä–µ", "—á—É–¥–æ–≤–æ", "–∫–ª–∞—Å–Ω–æ", "—Å—É–ø–µ—Ä", "–≤—ñ–¥–º—ñ–Ω–Ω–æ", "–≥–∞—Ä–Ω–æ", "–∫—Ä—É—Ç–æ", "üëç", "‚ù§Ô∏è", "üòä"]
        negative_words = ["–ø–æ–≥–∞–Ω–æ", "–∂–∞—Ö–ª–∏–≤–æ", "—Å—É–º–Ω–æ", "–¥—Ä–∞—Ç—É—î", "–±—ñ—Å–∏—Ç—åv", "üëé", "üò¢", "üò°"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            sentiment = "–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π"
            score = 0.7
        elif negative_count > positive_count:
            sentiment = "–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π" 
            score = 0.3
        else:
            sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π"
            score = 0.5
            
        return {
            "sentiment": sentiment,
            "score": score,
            "positive_count": positive_count,
            "negative_count": negative_count
        }

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
_analyzer_instance = None

def get_analyzer() -> LocalAnalyzer:
    """–û—Ç—Ä–∏–º—É—î –≥–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞ (Singleton)"""
    global _analyzer_instance
    if _analyzer_instance is None:
        _analyzer_instance = LocalAnalyzer()
    return _analyzer_instance

# –î–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –∑ —ñ—Å–Ω—É—é—á–∏–º –∫–æ–¥–æ–º
async def analyze_text_local(text: str) -> Dict[str, Any]:
    """–®–≤–∏–¥–∫–∏–π –ª–æ–∫–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É"""
    analyzer = get_analyzer()
    return await analyzer.analyze_message(text)

async def get_conversation_context(chat_id: int, messages: List[Dict], hours: int = 24) -> Dict[str, Any]:
    """–û—Ç—Ä–∏–º—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–æ–∑–º–æ–≤–∏ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –≤ Gemini"""
    analyzer = get_analyzer()
    
    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –±–∞—Ç—á –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
    current_analysis = await analyzer.analyze_conversation_batch(messages, chat_id)
    
    # –û—Ç—Ä–∏–º—É—î–º–æ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    historical_context = analyzer.get_chat_context_summary(chat_id, hours)
    
    return {
        "current_conversation": current_analysis,
        "historical_context": historical_context,
        "recommended_for_gemini": _prepare_gemini_context(current_analysis, historical_context)
    }

def _prepare_gemini_context(current: Dict, historical: Dict) -> str:
    """–ü—ñ–¥–≥–æ—Ç–æ–≤—É—î —Å—Ç–∏—Å–ª–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –≤ Gemini"""
    context_parts = []
    
    # –ü–æ—Ç–æ—á–Ω–∞ —Ä–æ–∑–º–æ–≤–∞
    if current.get("summary"):
        context_parts.append(f"–ü–æ—Ç–æ—á–Ω–∞ —Å–∏—Ç—É–∞—Ü—ñ—è: {current['summary']}")
    
    if current.get("dominant_emotion") != "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∏–π":
        context_parts.append(f"–ù–∞—Å—Ç—Ä—ñ–π: {current['dominant_emotion']}")
    
    if current.get("main_topics") and current["main_topics"][0] != "–∑–∞–≥–∞–ª—å–Ω–µ":
        context_parts.append(f"–¢–µ–º–∏: {', '.join(current['main_topics'][:2])}")
    
    # –Ü—Å—Ç–æ—Ä–∏—á–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    if historical.get("status") == "success":
        if historical.get("dominant_emotions"):
            context_parts.append(f"–ó–∞–≥–∞–ª—å–Ω–∏–π –Ω–∞—Å—Ç—Ä—ñ–π —á–∞—Ç—É: {', '.join(historical['dominant_emotions'][:2])}")
        
        if historical.get("main_topics"):
            context_parts.append(f"–ü–æ–ø—É–ª—è—Ä–Ω—ñ —Ç–µ–º–∏: {', '.join(historical['main_topics'][:3])}")
    
    return " | ".join(context_parts) if context_parts else "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ —Ä–æ–∑–º–æ–≤–∞"

# –ï–∫—Å–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü—ñ–π –¥–ª—è —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó
__all__ = [
    'LocalAnalyzer',
    'get_analyzer', 
    'analyze_text_local',
    'get_conversation_context'
]
