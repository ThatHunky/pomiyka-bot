"""
–§–∞–∑–∞ 2: –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π backup manager –∑ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—î—é
"""

import os
import shutil
import zipfile
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import asyncio
import hashlib

logger = logging.getLogger(__name__)

class AdvancedBackupManager:
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è"""
    
    def __init__(self, backup_dir: str = "backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        self.max_backups = int(os.getenv('BACKUP_MAX_COUNT', '10'))
        self.backup_interval_hours = int(os.getenv('BACKUP_INTERVAL_HOURS', '6'))
        self.compression_level = 6
        
        # –§–∞–π–ª–∏ —Ç–∞ –ø–∞–ø–∫–∏ –¥–ª—è backup
        self.backup_targets = [
            "data/",
            "bot/bot_config.py", 
            ".env",
            "requirements.txt",
            "docker-compose.yml",
            "Dockerfile"
        ]
        
        # –§–∞–π–ª–∏ –¥–ª—è –≤–∏–∫–ª—é—á–µ–Ω–Ω—è
        self.exclude_patterns = [
            "*.pyc",
            "__pycache__",
            "*.log",
            "*.tmp"
        ]
    
    async def create_backup(self, backup_type: str = "auto") -> Optional[str]:
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä–µ–∑–µ—Ä–≤–Ω–æ—ó –∫–æ–ø—ñ—ó
        
        Args:
            backup_type: —Ç–∏–ø backup ('auto', 'manual', 'pre-deploy')
        
        Returns:
            —à–ª—è—Ö –¥–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ–≥–æ backup —Ñ–∞–π–ª—É –∞–±–æ None –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"gryag_backup_{backup_type}_{timestamp}.zip"
            backup_path = self.backup_dir / backup_name
            
            print(f"üì¶ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è backup: {backup_name}")
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ metadata
            metadata = await self._create_metadata(backup_type)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ zip –∞—Ä—Ö—ñ–≤
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED, 
                               compresslevel=self.compression_level) as zipf:
                
                # –î–æ–¥–∞—î–º–æ metadata
                zipf.writestr("backup_metadata.json", 
                            json.dumps(metadata, indent=2, ensure_ascii=False))
                
                # –î–æ–¥–∞—î–º–æ —Ñ–∞–π–ª–∏ —Ç–∞ –ø–∞–ø–∫–∏
                for target in self.backup_targets:
                    await self._add_to_backup(zipf, target)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
            if await self._verify_backup(backup_path):
                # –û—á–∏—â—É—î–º–æ —Å—Ç–∞—Ä—ñ backup
                await self._cleanup_old_backups()
                
                file_size = backup_path.stat().st_size / (1024 * 1024)  # MB
                print(f"‚úÖ Backup —Å—Ç–≤–æ—Ä–µ–Ω–æ: {backup_name} ({file_size:.1f} MB)")
                
                # –õ–æ–≥—É—î–º–æ —É—Å–ø—ñ—à–Ω–∏–π backup
                await self._log_backup_event("created", backup_name, metadata)
                
                return str(backup_path)
            else:
                print(f"‚ùå Backup –ø–æ—à–∫–æ–¥–∂–µ–Ω–æ: {backup_name}")
                backup_path.unlink(missing_ok=True)
                return None
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è backup: {e}")
            print(f"üí• –ü–æ–º–∏–ª–∫–∞ backup: {e}")
            return None
    
    async def restore_backup(self, backup_path: str, 
                           restore_targets: Optional[List[str]] = None) -> bool:
        """
        –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ—ó –∫–æ–ø—ñ—ó
        
        Args:
            backup_path: —à–ª—è—Ö –¥–æ backup —Ñ–∞–π–ª—É
            restore_targets: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è (None = –≤—Å—ñ)
        
        Returns:
            True —è–∫—â–æ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —É—Å–ø—ñ—à–Ω–µ
        """
        try:
            backup_file = Path(backup_path)
            if not backup_file.exists():
                print(f"‚ùå Backup —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {backup_path}")
                return False
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
            if not await self._verify_backup(backup_file):
                print(f"‚ùå Backup –ø–æ—à–∫–æ–¥–∂–µ–Ω–æ: {backup_path}")
                return False
            
            print(f"üîÑ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ backup: {backup_file.name}")
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ backup –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å—Ç–∞–Ω—É –ø–µ—Ä–µ–¥ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è–º
            pre_restore_backup = await self.create_backup("pre-restore")
            
            with zipfile.ZipFile(backup_file, 'r') as zipf:
                # –ß–∏—Ç–∞—î–º–æ metadata
                metadata = json.loads(zipf.read("backup_metadata.json"))
                print(f"üìã Backup —Å—Ç–≤–æ—Ä–µ–Ω–æ: {metadata['timestamp']}")
                print(f"üìã –¢–∏–ø: {metadata['backup_type']}")
                
                # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ —Ñ–∞–π–ª–∏
                files_to_restore = restore_targets or zipf.namelist()
                
                for file_name in files_to_restore:
                    if file_name == "backup_metadata.json":
                        continue
                    
                    try:
                        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                        target_path = Path(file_name)
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ —Ñ–∞–π–ª
                        zipf.extract(file_name, ".")
                        print(f"‚úÖ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ: {file_name}")
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è {file_name}: {e}")
            
            await self._log_backup_event("restored", backup_file.name, metadata)
            print(f"üéâ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            
            if pre_restore_backup:
                print(f"üíæ Pre-restore backup: {Path(pre_restore_backup).name}")
            
            return True
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è backup: {e}")
            print(f"üí• –ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è: {e}")
            return False
    
    async def list_backups(self) -> List[Dict[str, Any]]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö backup"""
        backups = []
        
        for backup_file in self.backup_dir.glob("*.zip"):
            try:
                # –ß–∏—Ç–∞—î–º–æ metadata
                with zipfile.ZipFile(backup_file, 'r') as zipf:
                    metadata = json.loads(zipf.read("backup_metadata.json"))
                
                file_size = backup_file.stat().st_size / (1024 * 1024)  # MB
                
                backups.append({
                    'filename': backup_file.name,
                    'path': str(backup_file),
                    'size_mb': round(file_size, 1),
                    'created': metadata['timestamp'],
                    'type': metadata['backup_type'],
                    'bot_version': metadata.get('bot_version', 'Unknown'),
                    'file_count': metadata.get('file_count', 0)
                })
                
            except Exception as e:
                logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ metadata –¥–ª—è {backup_file}: {e}")
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –¥–∞—Ç–æ—é —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
        backups.sort(key=lambda x: x['created'], reverse=True)
        return backups
    
    async def get_backup_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ backup"""
        backups = await self.list_backups()
        
        if not backups:
            return {
                'total_backups': 0,
                'total_size_mb': 0,
                'latest_backup': None,
                'backup_types': {}
            }
        
        total_size = sum(b['size_mb'] for b in backups)
        backup_types = {}
        
        for backup in backups:
            backup_type = backup['type']
            backup_types[backup_type] = backup_types.get(backup_type, 0) + 1
        
        return {
            'total_backups': len(backups),
            'total_size_mb': round(total_size, 1),
            'latest_backup': backups[0],
            'backup_types': backup_types,
            'max_backups': self.max_backups,
            'backup_interval_hours': self.backup_interval_hours
        }
    
    async def auto_backup_check(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –ø–æ—Ç—Ä—ñ–±–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π backup"""
        backups = await self.list_backups()
        
        if not backups:
            return True  # –ù–µ–º–∞—î –∂–æ–¥–Ω–æ–≥–æ backup
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π auto backup
        auto_backups = [b for b in backups if b['type'] == 'auto']
        if not auto_backups:
            return True
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø—Ä–æ–π—à–æ–≤ —ñ–Ω—Ç–µ—Ä–≤–∞–ª
        latest_auto = datetime.fromisoformat(auto_backups[0]['created'])
        interval = timedelta(hours=self.backup_interval_hours)
        
        return datetime.now() - latest_auto >= interval
    
    async def _create_metadata(self, backup_type: str) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è metadata –¥–ª—è backup"""
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'backup_type': backup_type,
            'bot_version': '3.0',  # –ú–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
            'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}",
            'targets': self.backup_targets,
            'file_count': 0,
            'checksum': ''
        }
        
        # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
        try:
            if os.path.exists('.env'):
                with open('.env', 'r') as f:
                    env_content = f.read()
                    # –ù–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —á—É—Ç–ª–∏–≤—ñ –¥–∞–Ω—ñ, —Ç—ñ–ª—å–∫–∏ —Å–ø–∏—Å–æ–∫ –∫–ª—é—á—ñ–≤
                    env_keys = [line.split('=')[0] for line in env_content.split('\n') 
                              if '=' in line and not line.startswith('#')]
                    metadata['env_keys'] = env_keys
        except Exception:
            pass
        
        return metadata
    
    async def _add_to_backup(self, zipf: zipfile.ZipFile, target: str):
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É/–ø–∞–ø–∫–∏ –¥–æ backup"""
        target_path = Path(target)
        
        if not target_path.exists():
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ —ñ—Å–Ω—É—î): {target}")
            return
        
        if target_path.is_file():
            # –î–æ–¥–∞—î–º–æ —Ñ–∞–π–ª
            zipf.write(target_path, target_path.name)
            
        elif target_path.is_dir():
            # –î–æ–¥–∞—î–º–æ –≤—Å—é –ø–∞–ø–∫—É
            for file_path in target_path.rglob('*'):
                if file_path.is_file():
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ –≤–∏–∫–ª—é—á–µ–Ω–∏–π —Ñ–∞–π–ª
                    if not self._should_exclude(file_path):
                        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö
                        arc_name = file_path.relative_to(target_path.parent)
                        zipf.write(file_path, str(arc_name))
    
    def _should_exclude(self, file_path: Path) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–ª—é—á–∏—Ç–∏ —Ñ–∞–π–ª"""
        for pattern in self.exclude_patterns:
            if file_path.match(pattern):
                return True
        return False
    
    async def _verify_backup(self, backup_path: Path) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ backup"""
        try:
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –º–æ–∂–µ–º–æ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –≤—Å—ñ —Ñ–∞–π–ª–∏
                test_result = zipf.testzip()
                if test_result:
                    logger.error(f"Backup corrupted: {test_result}")
                    return False
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å metadata
                if "backup_metadata.json" not in zipf.namelist():
                    logger.error("Backup missing metadata")
                    return False
                
                return True
                
        except Exception as e:
            logger.error(f"Backup verification failed: {e}")
            return False
    
    async def _cleanup_old_backups(self):
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö backup"""
        backups = await self.list_backups()
        
        if len(backups) <= self.max_backups:
            return
        
        # –í–∏–¥–∞–ª—è—î–º–æ –Ω–∞–π—Å—Ç–∞—Ä—ñ—à—ñ backup
        backups_to_delete = backups[self.max_backups:]
        
        for backup in backups_to_delete:
            try:
                Path(backup['path']).unlink()
                print(f"üóëÔ∏è  –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä–∏–π backup: {backup['filename']}")
            except Exception as e:
                logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥–∞–ª–∏—Ç–∏ backup {backup['filename']}: {e}")
    
    async def _log_backup_event(self, event_type: str, backup_name: str, metadata: Dict[str, Any]):
        """–õ–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–¥—ñ–π backup"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'event': event_type,
            'backup_name': backup_name,
            'metadata': metadata
        }
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –ª–æ–≥ —Ñ–∞–π–ª
        log_file = self.backup_dir / "backup_log.json"
        
        try:
            # –ß–∏—Ç–∞—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as f:
                    log_data = json.load(f)
            else:
                log_data = []
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å
            log_data.append(log_entry)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ 100 –∑–∞–ø–∏—Å—ñ–≤
            if len(log_data) > 100:
                log_data = log_data[-100:]
            
            # –ó–∞–ø–∏—Å—É—î–º–æ –ª–æ–≥
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø–∏—Å–∞—Ç–∏ backup –ª–æ–≥: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –µ–∫–∑–µ–º–ø–ª—è—Ä
backup_manager = AdvancedBackupManager()

# –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
async def create_backup(backup_type: str = "manual") -> Optional[str]:
    """–®–≤–∏–¥–∫–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è backup"""
    return await backup_manager.create_backup(backup_type)

def backup_database():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ"""
    try:
        return asyncio.run(create_backup("auto"))
    except Exception as e:
        logger.error(f"Backup failed: {e}")
        return None

async def auto_backup_if_needed():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π backup —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω"""
    if await backup_manager.auto_backup_check():
        return await backup_manager.create_backup("auto")
    return None

if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è backup manager
    async def test_backup():
        print("üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è backup manager...")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è backup
        backup_path = await backup_manager.create_backup("test")
        if backup_path:
            print(f"‚úÖ Backup —Å—Ç–≤–æ—Ä–µ–Ω–æ: {backup_path}")
            
            # –°–ø–∏—Å–æ–∫ backup
            backups = await backup_manager.list_backups()
            print(f"üìã –ó–Ω–∞–π–¥–µ–Ω–æ {len(backups)} backup")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = await backup_manager.get_backup_stats()
            print(f"üìä –í—Å—å–æ–≥–æ backup: {stats['total_backups']}")
            print(f"üìä –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {stats['total_size_mb']} MB")
        
    asyncio.run(test_backup())
